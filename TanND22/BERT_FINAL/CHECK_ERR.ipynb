{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a81c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subword(tokenizer, X,  Y):\n",
    "        '''\n",
    "        input:\n",
    "            sentence = ['Phạm', 'Văn', 'Mạnh']\n",
    "            text_labels = ['B-PER', 'I-PER','I-PER']\n",
    "\n",
    "        output: \n",
    "            ['Phạm', 'Văn', 'M', '##ạnh'],\n",
    "            ['B-PER', 'I-PER', 'I-PER', 'I-PER']\n",
    "        '''\n",
    "        tokenized_sentence = []\n",
    "        labels = []\n",
    "        for word, label in zip(X, Y):\n",
    "          \n",
    "          subwords = tokenizer.tokenize(word)\n",
    "          tokenized_sentence.extend(subwords)\n",
    "          labels.extend([label] * len(subwords))\n",
    "        return tokenized_sentence, labels\n",
    "\n",
    "def padding_data(tokenizer,X_subword,y_subword,MAXLEN,tag2idx):\n",
    "        '''\n",
    "            input:\n",
    "                X = [['Phạm', 'Văn', 'M', '##ạnh',..],....]\n",
    "                Y = [['B-PER', 'I-PER','I-PER','I-PER',..],...]\n",
    "\n",
    "            output: \n",
    "            [[10,20,30,40,0,0,0,0,0,0,0,0...],...],\n",
    "            [[1, 2,3,4,5,5,5,5,5,5,5,5,5,...],...]\n",
    "        '''\n",
    "        X_padding = pad_sequences([tokenizer.convert_tokens_to_ids(X_subword)],\n",
    "                          maxlen=MAXLEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "        y_padding = pad_sequences([[tag2idx.get(l) for l in y_subword]],\n",
    "                        maxlen=MAXLEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                        dtype=\"long\", truncating=\"post\")\n",
    "        \n",
    "        return X_padding,y_padding\n",
    "\n",
    "def predict_data_train(model, tokenizer, tag_values, dataset , idx, tag2idx, MAXLEN, device):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            - model:\n",
    "            - tokenizer:\n",
    "            - tag_value: ['O', 'PER', 'LOC',...]\n",
    "            - dataset: [[('anh','O'),...]]\n",
    "            - idx: index in dataset\n",
    "            - tag2idx: {'PER': 0, 'LOC': 1, 'ORG': 2, 'MISC': 3, 'O': 4, 'PAD': 5}\n",
    "            - MAXLEN: 256,512,...\n",
    "    \"\"\"\n",
    "    X = [[w for w,_ in sq] for  sq in dataset][idx]\n",
    "    Y = [[t for _,t in sq] for  sq in dataset][idx]\n",
    "    X_Sub, Y_Sub = add_subword(tokenizer, X, Y)\n",
    "    X_padding, Y_padding = padding_data(tokenizer,X_Sub,Y_Sub,MAXLEN,tag2idx)\n",
    "    input_ids_tensor = torch.tensor(X_padding).type(torch.LongTensor).to(device)\n",
    "    input_mask = [[float(i != 0.0) for i in ii] for ii in X_padding]\n",
    "    input_mask_tensor = torch.tensor(input_mask).type(torch.LongTensor).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward_custom(input_ids_tensor, attention_mask = input_mask_tensor)\n",
    "    logits = outputs[0].detach().cpu().numpy()\n",
    "\n",
    "        #Precroces subword\n",
    "\n",
    "    len_subword = sum(X_padding[0] != 0)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor[0].to('cpu').numpy())[:len_subword]\n",
    "    predict = np.argmax(logits, axis=2)[0][:len_subword]\n",
    "\n",
    "    tags_predict = [tag_values[i]  for i in  predict]\n",
    "    tags_true = [tag_values[i]  for i in  Y_padding[0]]\n",
    "    y_predict = []\n",
    "    words = []\n",
    "    y_true = []\n",
    "    for index in range(len(tokens)):\n",
    "        if \"##\" not in tokens[index]:\n",
    "            y_predict.append(tags_predict[index])\n",
    "            y_true.append(tags_true[index])\n",
    "            words.append(tokens[index])\n",
    "        else:\n",
    "            words[-1] = words[-1] + tokens[index].replace(\"##\",\"\")\n",
    "    return [(w,'?',t) for w,t in zip(words,y_true)], y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c3f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
