{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from utils.processing_data import is_IP"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "text = \"096.132.142.2\"\n",
    "post_processing(text, [(\"096.132.142.2\", \"PHONENUMBER\")])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('096.132.142.2', 'PHONENUMBER')]"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "\n",
    "import string\n",
    "import unicodedata\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    dictt = {'â„¢': ' ', 'â€˜': \"'\", 'Â®': ' ', 'Ã—': ' ', 'ðŸ˜€': ' ', 'â€‘': ' - ', 'Ì': ' ', 'â€”': ' - ', 'Ì£': ' ', 'â€“': ' - ', '`': \"'\",\\\n",
    "             'â€œ': '\"', 'Ì‰': ' ','â€™': \"'\", 'Ìƒ': ' ', '\\u200b': ' ', 'Ì€': ' ', 'â€': '\"', 'â€¦': '...', '\\ufeff': ' ', 'â€³': '\"'}\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    res = ''\n",
    "    for i in text:\n",
    "        if i.isalnum() or i in string.punctuation or i == ' ':\n",
    "            res += i\n",
    "        elif i in dictt:\n",
    "            res += dictt[i]\n",
    "    return res\n",
    "  \n",
    "\n",
    "def merge_word(sent, pred_out):\n",
    "  '''\n",
    "    :sent: is input sentences (hanlded pre-processing). example: 'pham van manh have email ( pvm26042000@gmail.com ) ....'\n",
    "    :out : is input of predict, is list tuple. example: [('pham', 'O'), ('van', 'O'), ('manh', 'O'), ('have', 'O'), ('email', 'O'), ('(', 'O'),  ('pvm26042000', 'EMAIL'), ('@', 'EMAIL'),('gmail', 'EMAIL'), ('.', 'EMAIL'),('com', 'EMAIL'),(')', 'O'),('....', 'O')]\n",
    "  '''\n",
    "  out_merged = []\n",
    "  parts = sent.split()\n",
    "  for index in range(0, len(parts)):\n",
    "    word = parts[index]\n",
    "\n",
    "    \n",
    "    for jndex in range(1, len(pred_out) + 1):\n",
    "      token = pred_out[0:jndex]\n",
    "      ws_token, _ = list(zip(*token))\n",
    "      word_token = \"\".join(ws_token)\n",
    "  \n",
    "      if word_token == word:\n",
    "        if len(token) == 1:\n",
    "          out_merged.append(token[0])\n",
    "        elif len(token) > 1:\n",
    "          a, b = list(zip(*token))\n",
    "          word_merged = \"\".join(a)\n",
    "          l_merged = decide_label((word_merged, b))\n",
    "          out_merged.append(l_merged)\n",
    "        pred_out = pred_out[jndex:]\n",
    "        break\n",
    "  return out_merged\n",
    "\n",
    "def post_processing(origin_sentence, out_predict):\n",
    "\n",
    "  out_merged = merge_word(origin_sentence, out_predict)\n",
    "  datas_trained = post_process_email_url(out_merged)\n",
    "\n",
    "  indexs = []\n",
    "  for index in range(len(datas_trained)):\n",
    "    token = datas_trained[index]\n",
    "    if token[1] == \"LOCATION\" or token[1] == \"ADDRESS\" :\n",
    "      indexs.append(index)\n",
    "\n",
    "  if len(indexs) != 0:\n",
    "    gr_indexs = cluster(indexs, 2)\n",
    "    \n",
    "    print(gr_indexs)\n",
    "    for index in gr_indexs:\n",
    "      string, label = list(zip(*datas_trained[index[0]: index[-1] + 1]))\n",
    "\n",
    "      if is_ADDRESS(string, label) == True:\n",
    "        for i in range(index[0], index[-1] + 1):\n",
    "          datas_trained[i] =(datas_trained[i][0], \"ADDRESS\")\n",
    "  return datas_trained\n",
    "\n",
    "def cluster(data, maxgap):\n",
    "    '''Arrange data into groups where successive elements\n",
    "       differ by no more than *maxgap*\n",
    "\n",
    "        >>> cluster([1, 6, 9, 100, 102, 105, 109, 134, 139], maxgap=10)\n",
    "        [[1, 6, 9], [100, 102, 105, 109], [134, 139]]\n",
    "\n",
    "        >>> cluster([1, 6, 9, 99, 100, 102, 105, 134, 139, 141], maxgap=10)\n",
    "        [[1, 6, 9], [99, 100, 102, 105], [134, 139, 141]]\n",
    "\n",
    "    '''\n",
    "    data.sort()\n",
    "    groups = [[data[0]]]\n",
    "    for x in data[1:]:\n",
    "        if abs(x - groups[-1][-1]) <= maxgap:\n",
    "            groups[-1].append(x)\n",
    "        else:\n",
    "            groups.append([x])\n",
    "    return groups\n",
    "  \n",
    "\n",
    "\n",
    "def has_numbers(inputString):\n",
    "  parts = inputString.split()\n",
    "  # print(parts)\n",
    "  for i in range(len(parts)):\n",
    "    part = parts[i]\n",
    "    for char in part:\n",
    "      if char.isdigit():\n",
    "        # print(i)\n",
    "        if i > 0 and parts[i-1].lower() in [\"quáº­n\", \"q.\"]:\n",
    "          return False\n",
    "        else:\n",
    "          return True\n",
    "  return False\n",
    "\n",
    "def is_ADDRESS(string, label):\n",
    "\n",
    "  level = [\"sá»‘\", \"lÃ´\", \"km\",\"quá»‘c_lá»™\",\"Ä‘áº¡i_lá»™\",\"kcn\", \"Ä‘Æ°á»ng\",\"tá»•\", \"ngÃµ\", \"toÃ \", \"ngÃ¡ch\", \"háº»m\",\"kiá»‡t\", \"chung_cÆ°\", \"áº¥p\" ,\"thÃ´n\", \"khu\",\"phá»‘\" , \"quáº­n\", \"phÆ°á»ng\", \"xÃ£\", \"thá»‹_xÃ£\",\"huyá»‡n\", \"thÃ nh_phá»‘\", \"tp\", \"tá»‰nh\" ]\n",
    "  level_0 ={'status': True,'keywords': [\"toÃ \", \"chung_cÆ°\", \"sá»‘\", \"lÃ´\", \"kcn\", \"km\", \"quá»‘c_lá»™\", \"Ä‘áº¡i_lá»™\"] }\n",
    "  level_1 = {'status': True, 'keywords': [ \"ngÃµ\", \"ngÃ¡ch\", \"háº»m\",\"kiá»‡t\",]}\n",
    "  level_2 = {'status': True, 'keywords':[\"áº¥p\" ,\"thÃ´n\", \"khu\",\"phá»‘\" , \"quáº­n\", \"phÆ°á»ng\", \"xÃ£\", \"tá»•\", \"dÃ¢n_phá»‘\", \"Ä‘Æ°á»ng\"]}\n",
    "  level_3 = {'status': True,'keywords':[\"thá»‹\",\"huyá»‡n\"]}\n",
    "  level_4 = {'status': True,'keywords':[\"thÃ nh_phá»‘\", \"tp\", \"tá»‰nh\"]}\n",
    "  index_not_dau_phay = [i for i, e in enumerate(label) if e == \"O\"]\n",
    "\n",
    "  uy_tin = 0\n",
    "  string_loc = \" \".join(string)\n",
    "\n",
    "  if 'ADDRESS' in label:\n",
    "    uy_tin += 0.1\n",
    "\n",
    "  if has_numbers(string_loc):\n",
    "    uy_tin += 0.2\n",
    "  \n",
    "  for i in index_not_dau_phay:\n",
    "      if string[i] not in [\",\", \"-\"]:\n",
    "        uy_tin -= 0.05\n",
    "      else:\n",
    "        if string[i] == \",\":\n",
    "          uy_tin += 0.02\n",
    "        if string[i] == \"-\":\n",
    "          uy_tin += 0.05\n",
    "\n",
    "  \n",
    "\n",
    "  parts =  ViPosTagger.postagging(ViTokenizer.tokenize(string_loc))[0]\n",
    "\n",
    "  for seg_word in parts:\n",
    "    if seg_word.lower() in level:\n",
    "\n",
    "      if seg_word.lower() in level_0['keywords'] and level_0['status'] == True:\n",
    "        uy_tin += 0.075\n",
    "        level_0['status'] = False\n",
    "\n",
    "      if seg_word.lower() in level_1['keywords'] and level_1['status'] == True:\n",
    "        uy_tin += 0.05\n",
    "        level_1['status'] = False\n",
    "\n",
    "      elif seg_word.lower()  in level_2['keywords'] and level_2['status'] == True:\n",
    "        uy_tin += 0.025\n",
    "        level_2['status'] = False\n",
    "      elif seg_word.lower() in  level_3['keywords'] and level_3['status'] == True:\n",
    "   \n",
    "        uy_tin += 0.015\n",
    "        level_3['status'] = False\n",
    "      elif seg_word.lower() in level_4['keywords'] and level_4['status'] == True:\n",
    "     \n",
    "        uy_tin += 0.01\n",
    "        level_4['status'] = False\n",
    "\n",
    "  if uy_tin >= 0.3:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "\n",
    "def decide_label(part):\n",
    "  word = part[0]\n",
    "  labels = part[1]\n",
    "  return (word, max(labels))\n",
    "\n",
    "\n",
    "import re\n",
    "def constain_alpha(token):\n",
    "\n",
    "  for character in token:\n",
    "\n",
    "    is_letter = character.isalpha()\n",
    "    if is_letter == True:\n",
    "      return True\n",
    "  \n",
    "  return False\n",
    "\n",
    "def is_URL(token):\n",
    "    token = token.lower()\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    if constain_alpha(token) == True:\n",
    "      domain = re.findall(r'\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b', token)\n",
    "      \n",
    "      if len(domain) != 0:\n",
    "          index_start_domain = token.find(domain[0]) + index\n",
    "          if token.find(domain[0]) == 0:\n",
    "              index_end_domain = index_start_domain + len(token)\n",
    "          else:\n",
    "              index_end_domain = index_start_domain + len(domain[0])\n",
    "          indexs.append((index_start_domain, index_end_domain))\n",
    "      index += len(token) + 1\n",
    "    return indexs\n",
    "\n",
    "def is_Email(token):\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    for word in token.split(\" \"):\n",
    "        # print(word)\n",
    "        emails = re.findall(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", word)\n",
    "        # print(emails)\n",
    "        if len(emails) != 0:\n",
    "            index_start_email = word.find(emails[0]) + index\n",
    "            \n",
    "            index_end_email = index_start_email + len(emails[0])\n",
    "            \n",
    "            indexs.append((index_start_email, index_end_email))\n",
    "        index += len(word) + 1\n",
    "    return indexs\n",
    "def is_IP(token):\n",
    "    index = 0\n",
    "    indexs = []\n",
    "\n",
    "    # print(word)\n",
    "    emails = re.findall(r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\", token)\n",
    "    # print(emails)\n",
    "    if len(emails) != 0:\n",
    "        index_start_email = token.find(emails[0]) + index\n",
    "        \n",
    "        index_end_email = index_start_email + len(emails[0])\n",
    "        \n",
    "        indexs.append((index_start_email, index_end_email))\n",
    "    index += len(token) + 1\n",
    "    return indexs\n",
    "\n",
    "def post_process_email_url(datas):\n",
    "  black_word = [\"tp.hcm\"]\n",
    "  datas_trained = []\n",
    "  for i in range(len(datas)):\n",
    "    data = datas[i]\n",
    "\n",
    "      # check predict email\n",
    "    if data[1] == 'EMAIL':\n",
    "        check = is_Email(data[0])\n",
    "        if len(check) == 0:\n",
    "          data = (data[0], 'O')\n",
    "    \n",
    "    elif data[1] == 'URL':\n",
    "        check = is_URL(data[0])\n",
    "\n",
    "        if len(check) == 0 or  check[0][1] - check[0][0]!= len(data[0]):\n",
    "        \n",
    "          data = (data[0], 'O')\n",
    "    \n",
    "    elif data[1] == 'IP':\n",
    "        check = is_IP(data[0])\n",
    "        print(check[0][1] - check[0][0]!= len(data[0]))\n",
    "        if len(check) == 0 or  check[0][1] - check[0][0]!= len(data[0]):\n",
    "          if data[0].isalnum():\n",
    "            data = (data[0], 'QUANTITY')\n",
    "          else:\n",
    "            data = (data[0], 'O')\n",
    "\n",
    "    # elif data[1] == 'PHONENUMBER':\n",
    "    #     check_ip = is_IP(data[0])\n",
    "    #     try:\n",
    "    #       if len(check_ip) > 0 and check_ip[0][1] - check_ip[0][0]== len(data[0]):\n",
    "    #         data = (data[0], 'IP')\n",
    "    #     except:\n",
    "    #       print(\"ERROR:{}\".format(data))\n",
    "    #       # return\n",
    "\n",
    "    elif data[1] in ['O'] and data[1].lower() not in black_word:\n",
    "        # print(data[0])\n",
    "        check_url = is_URL(data[0])\n",
    "        check_email= is_Email(data[0])\n",
    "\n",
    "        if len(check_url) > 0 and  check_url[0][1] - check_url[0][0] == len(data[0]):\n",
    "\n",
    "          data = (data[0], 'URL')\n",
    "\n",
    "        elif len(check_email) > 0:\n",
    "          data = (data[0], 'EMAIL')\n",
    "      \n",
    "    datas_trained.append(data)\n",
    "  return datas_trained"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "is_IP(\"192.121.111.2\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert .Coll sang DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def read_conll(in_file, lowercase=False, max_example=None):\n",
    "    f_name = in_file.split(\"/\")[-1].replace(\".conll\", \"\")\n",
    "    examples = []\n",
    "    \n",
    "    with open(in_file) as f:\n",
    "        n_sents, word, label1, label2, label3= [], [], [], [], []\n",
    "        n_sent = 0\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            sp = line.strip().split('\\t')\n",
    "            if sp[0] != '_':\n",
    "                if len(sp) >= 4:\n",
    "                    part = sp[3].split('-', 1)\n",
    "                    if len(part) > 1:\n",
    "                        postion = part[0]\n",
    "                        label_name = part[1]\n",
    "                    else:\n",
    "                        postion = 'default'\n",
    "                        label_name = part[0]\n",
    "\n",
    "                    if postion == 'I' and label1[-1] == label_name:\n",
    "                        if label_name not in ['URL', 'EMAIL', 'PHONENUMBER']:\n",
    "                            word[-1] = word[-1] + ' ' + sp[0]\n",
    "                        else:\n",
    "                            word[-1] = word[-1] + sp[0]\n",
    "\n",
    "                    else:\n",
    "                        word.append(sp[0])\n",
    "                        label1.append(label_name)\n",
    "                        label2.append('?')\n",
    "                        label3.append('?')\n",
    "                        n_sents.append(n_sent)\n",
    "                        \n",
    "                if len(sp) == 5:\n",
    "                    label2[-1] = sp[4]\n",
    "                if len(sp) == 6:\n",
    "                    label3[-1] = sp[5]\n",
    "                n_sent += 1  \n",
    "     \n",
    "            # save sentence to examples\n",
    "            if sp[0] == '':\n",
    "                if word[-1] != '.':\n",
    "                    word.append('.')\n",
    "                    label1.append('O')\n",
    "                    label2.append('END')\n",
    "                    label3.append('END')\n",
    "                    n_sents.append(n_sent)\n",
    "\n",
    "                examples.append({'f_name':f_name,'n_sent':n_sents, 'word': word, 'label1': label1, 'label2': label2, 'label3': label3})\n",
    "                n_sent = 0\n",
    "           \n",
    "            \n",
    "    return examples "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "'I-PHONENUMBER-MANH'.split('-', 1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I', 'PHONENUMBER-MANH']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "dff = read_conll('/Users/phamvanmanh/Desktop/machine/final_vlsp/converted/train-converted/00_add_0269.conll')\n",
    "df1 = pd.DataFrame(dff[0])\n",
    "df1[df1['label1'] == 'PHONENUMBER']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>n_sent</th>\n",
       "      <th>word</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>00_add_0269</td>\n",
       "      <td>58</td>\n",
       "      <td>0987673608</td>\n",
       "      <td>PHONENUMBER</td>\n",
       "      <td>O</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f_name  n_sent        word       label1 label2 label3\n",
       "399  00_add_0269      58  0987673608  PHONENUMBER      O      ?"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def to_zip(df, parent_path, fold_name):\n",
    "\n",
    "    compression_opts = dict(method='zip',\n",
    "                                archive_name=fold_name + '.csv')  \n",
    "    df.to_csv( parent_path + fold_name + '.zip', index=False,\n",
    "                compression=compression_opts) \n",
    "    return print(fold_name + \" saved done!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import os\n",
    "\n",
    "def convert_Dataframe(in_folder, out_folder):\n",
    "    \n",
    "    sub_folders = [name for name in os.listdir(in_folder) if os.path.isdir(os.path.join(in_folder, name))]\n",
    "    print(sub_folders)\n",
    "    for n_folder in sub_folders:\n",
    "        df = pd.DataFrame(columns = ['f_name','n_sent', 'word', 'label1', 'label2', 'label3'])\n",
    "        path = in_folder + n_folder\n",
    "        sub_files = os.listdir(path)\n",
    "    \n",
    "  \n",
    "        for s_file in sub_files:\n",
    "            try:\n",
    "                f_path = path + '/' + s_file\n",
    "                data = read_conll(f_path)\n",
    "                df1 = pd.DataFrame(data[0])\n",
    "                df = df.append(df1)\n",
    "            except  Exception as e: print('error read_conll: {}, {}'.format(e, f_path))\n",
    "\n",
    "        try:\n",
    "            to_zip(df, out_folder, n_folder)\n",
    "        except  Exception as e: print('error to_zip: {}'.format(e))\n",
    "        \n",
    "\n",
    "\n",
    "    return 'done!'\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "x"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "out_folder = '/Users/phamvanmanh/Desktop/machine/final_vlsp/converted_to_dataframe/'\n",
    "in_folder = '/Users/phamvanmanh/Desktop/machine/final_vlsp/converted/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "convert_Dataframe(in_folder, out_folder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['test-converted', 'dev-converted', 'train-converted']\n",
      "test-converted saved done!\n",
      "dev-converted saved done!\n",
      "train-converted saved done!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'done!'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('mlenv': venv)"
  },
  "interpreter": {
   "hash": "64eef7adee6260e839badb1decb60292ef8dcf6f0c9c7bd538f98ad735b6d354"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}