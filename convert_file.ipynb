{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from utils.processing_data import is_IP"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/phamvanmanh/Documents/GitHub/NER_MOCKPROJECT/utils/address.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "len(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "w, n = list(zip(*data[0]))\n",
    "post_processing(\" \".join(w), data[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('sống', 'O'),\n",
       " ('ở', 'O'),\n",
       " ('đường', 'ADDRESS'),\n",
       " ('Nguyễn', 'LOCATION'),\n",
       " ('văn', 'LOCATION'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Phường', 'ADDRESS'),\n",
       " ('7', 'ADDRESS'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Thành', 'ADDRESS'),\n",
       " ('Phố', 'ADDRESS'),\n",
       " ('HCM', 'ADDRESS')]"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2712009808.py, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/w7/_vsgqkws5y93szpqyrb2mqw80000gn/T/ipykernel_2557/2712009808.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    preprocess_email_url(\" \".join(data[0]), data[1]\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "import string\n",
    "import unicodedata\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_email_url(datas):\n",
    "  datas_trained = []\n",
    "  for i in range(len(datas)):\n",
    "    data = datas[i]\n",
    "\n",
    "    if data[1] == 'EMAIL':\n",
    "      check = is_Email(data[0])\n",
    "      if len(check) == 0:\n",
    "        data = (data[0], 'O')\n",
    "    \n",
    "    if data[1] != 'EMAIL' and  data[1] != 'URL': #(url, org, loc, o,.....)\n",
    "      check = is_Email(data[0])\n",
    "      if len(check) > 0:\n",
    "        data = (data[0], 'EMAIL')\n",
    "\n",
    "  \n",
    "\n",
    "    if data[1] == \"URL\":\n",
    "      # print(data[0])\n",
    "      check = is_URL(data[0])\n",
    "      if len(check) > 0 and  check[0][1] - check[0][0] == len(data[0]):\n",
    "        data = (data[0], 'URL')\n",
    "      else: \n",
    "        data = (data[0], 'O')\n",
    "      \n",
    "    try:\n",
    "      if data[1] != 'URL' and data[1] != 'EMAIL':\n",
    "        check = is_URL(data[0])\n",
    "        if len(check) > 0 and  check[0][1] - check[0][0] == len(data[0]):\n",
    "          data = (data[0], 'URL')\n",
    "    except:\n",
    "      print(check)\n",
    "    datas_trained.append(data)\n",
    "  return datas_trained\n",
    "    \n",
    "# sent = 'pham van manh have email ( pvm26042000@gmail.com ) ....'\n",
    "# out = [('pham', 'O'), ('van', 'O'), ('manh', 'O'), ('have', 'O'), ('email', 'O'), ('(', 'O'),  ('pvm26042000', 'EMAIL'), ('@', 'EMAIL'),('gmail', 'EMAIL'), ('.', 'EMAIL'),('com', 'EMAIL'),(')', 'O'),('....', 'O')]\n",
    "\n",
    "def merge_word(sent, pred_out):\n",
    "  '''\n",
    "    :sent: is input sentences (hanlded pre-processing). example: 'pham van manh have email ( pvm26042000@gmail.com ) ....'\n",
    "    :out : is input of predict, is list tuple. example: [('pham', 'O'), ('van', 'O'), ('manh', 'O'), ('have', 'O'), ('email', 'O'), ('(', 'O'),  ('pvm26042000', 'EMAIL'), ('@', 'EMAIL'),('gmail', 'EMAIL'), ('.', 'EMAIL'),('com', 'EMAIL'),(')', 'O'),('....', 'O')]\n",
    "  '''\n",
    "  out_merged = []\n",
    "  parts = sent.split()\n",
    "  for index in range(0, len(parts)):\n",
    "    word = parts[index]\n",
    "\n",
    "    \n",
    "    for jndex in range(1, len(pred_out) + 1):\n",
    "      token = pred_out[0:jndex]\n",
    "      ws_token, _ = list(zip(*token))\n",
    "      word_token = \"\".join(ws_token)\n",
    "   \n",
    "      if word_token == word:\n",
    "        if len(token) == 1:\n",
    "          out_merged.append(token[0])\n",
    "        elif len(token) > 1:\n",
    "          a, b = list(zip(*token))\n",
    "          word_merged = \"\".join(a)\n",
    "          l_merged = decide_label((word_merged, b))\n",
    "          out_merged.append(l_merged)\n",
    "        pred_out = pred_out[jndex:]\n",
    "        break\n",
    "  return out_merged\n",
    "\n",
    "def post_processing(origin_sentence, out_predict):\n",
    "\n",
    "  out_merged = merge_word(origin_sentence, out_predict)\n",
    "  datas_trained = post_process_email_url(out_merged)\n",
    "\n",
    "  indexs = []\n",
    "  for index in range(len(datas_trained)):\n",
    "    token = datas_trained[index]\n",
    "    if token[1] == \"LOCATION\" or token[1] == \"ADDRESS\" :\n",
    "      indexs.append(index)\n",
    "\n",
    "  if len(indexs) != 0:\n",
    "    gr_indexs = cluster(indexs, 3)\n",
    "\n",
    "\n",
    "    for index in gr_indexs:\n",
    "      string, label = list(zip(*datas_trained[index[0]: index[-1] + 1]))\n",
    "      # print(string, label)\n",
    "      if is_ADDRESS(string, label) == True:\n",
    "        for i in range(index[0], index[-1] + 1):\n",
    "          datas_trained[i] =(datas_trained[i][0], \"ADDRESS\")\n",
    "      else:\n",
    "        for i in range(index[0], index[-1] + 1):\n",
    "          if datas_trained[i][0] == ',':\n",
    "            datas_trained[i] = (datas_trained[i][0], \"O\")\n",
    "          else:\n",
    "            datas_trained[i] =(datas_trained[i][0], \"LOCATION\")\n",
    "  return datas_trained\n",
    "\n",
    "def cluster(data, maxgap):\n",
    "    '''Arrange data into groups where successive elements\n",
    "       differ by no more than *maxgap*\n",
    "        >>> cluster([1, 6, 9, 100, 102, 105, 109, 134, 139], maxgap=10)\n",
    "        [[1, 6, 9], [100, 102, 105, 109], [134, 139]]\n",
    "        >>> cluster([1, 6, 9, 99, 100, 102, 105, 134, 139, 141], maxgap=10)\n",
    "        [[1, 6, 9], [99, 100, 102, 105], [134, 139, 141]]\n",
    "    '''\n",
    "    data.sort()\n",
    "    groups = [[data[0]]]\n",
    "    for x in data[1:]:\n",
    "        if abs(x - groups[-1][-1]) <= maxgap:\n",
    "            groups[-1].append(x)\n",
    "        else:\n",
    "            groups.append([x])\n",
    "    return groups\n",
    "  \n",
    "\n",
    "\n",
    "def has_numbers(inputString):\n",
    "  parts = inputString.split()\n",
    "  for i in range(len(parts)):\n",
    "    part = parts[i]\n",
    "    for char in part:\n",
    "      if char.isdigit():\n",
    "        if i > 0 and parts[i-1].lower() in [\"quận\", \"q.\"]:\n",
    "          return False\n",
    "        else:\n",
    "          return True\n",
    "  return False\n",
    "\n",
    "def is_ADDRESS(string, label):\n",
    "\n",
    "  uy_tin = 0\n",
    "  string_loc = \" \".join(string)\n",
    "\n",
    "  level = [\"số\", \"lô\", \"km\",\"quốc_lộ\",\"đại_lộ\",\"kcn\", \"đường\",\"tổ\", \"ngõ\", \"toà\", \"ngách\", \"hẻm\",\"kiệt\", \"chung_cư\", \"số_nhà\",\"ấp\" ,\"thôn\", \"khu\",\"phố\" , \"quận\", \"phường\", \"xã\", \"thị_xã\",\"huyện\", \"thành_phố\", \"tp\", \"tỉnh\" ]\n",
    "  level_0 ={'status': True,'keywords': [\"toà\", \"chung_cư\", \"số\", \"lô\", \"số_nhà\"] }\n",
    "  level_1 = {'status': True, 'keywords': [ \"ngõ\", \"ngách\", \"hẻm\",\"kiệt\",\"kcn\", \"km\"]}\n",
    "  level_2 = {'status': True, 'keywords':[\"ấp\" ,\"thôn\", \"khu\",\"phố\" , \"quận\", \"phường\", \"xã\", \"tổ\", \"dân_phố\", \"đường\", \"quốc_lộ\", \"đại_lộ\"]}\n",
    "  level_3 = {'status': True,'keywords':[\"thị\",\"huyện\"]}\n",
    "  level_4 = {'status': True,'keywords':[\"thành_phố\", \"tp\", \"tỉnh\"]}\n",
    "\n",
    "  parts =  ViPosTagger.postagging(ViTokenizer.tokenize(string_loc))[0]\n",
    "\n",
    "  for index in range(len(parts)):\n",
    "    seg_word = parts[index]\n",
    "    if index == 0:\n",
    "      if has_numbers(seg_word.split(\" \")[0]):\n",
    "        uy_tin += 0.3\n",
    "\n",
    "    if seg_word.lower() in level:\n",
    " \n",
    "      if seg_word.lower() in level_0['keywords'] and level_0['status'] == True:\n",
    "        uy_tin += 0.25\n",
    "        level_0['status'] = False\n",
    "\n",
    "      elif seg_word.lower() in level_1['keywords'] and level_1['status'] == True:\n",
    "        uy_tin += 0.075\n",
    "        level_1['status'] = False\n",
    "\n",
    "      elif seg_word.lower()  in level_2['keywords'] and level_2['status'] == True:\n",
    "        uy_tin += 0.025\n",
    "        level_2['status'] = False\n",
    "      elif seg_word.lower() in  level_3['keywords'] and level_3['status'] == True:\n",
    "   \n",
    "        uy_tin += 0.015\n",
    "        level_3['status'] = False\n",
    "      elif seg_word.lower() in level_4['keywords'] and level_4['status'] == True:\n",
    "     \n",
    "        uy_tin += 0.01\n",
    "        level_4['status'] = False\n",
    "  print(uy_tin)\n",
    "  if uy_tin >= 0.3:\n",
    "    return True\n",
    "  \n",
    "  return False\n",
    "\n",
    "\n",
    "def decide_label(part):\n",
    "  word = part[0]\n",
    "  labels = part[1]\n",
    "  return (word, max(labels))\n",
    "\n",
    "\n",
    "import re\n",
    "def constain_alpha(token):\n",
    "\n",
    "  for character in token:\n",
    "\n",
    "    is_letter = character.isalpha()\n",
    "    if is_letter == True:\n",
    "      return True\n",
    "  \n",
    "  return False\n",
    "\n",
    "def is_URL(token):\n",
    "    token = token.lower()\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    if constain_alpha(token) == True:\n",
    " \n",
    "      \n",
    "      # print(word)\n",
    "      domain = re.findall(r'\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b', token)\n",
    "      \n",
    "      if len(domain) != 0:\n",
    "          index_start_domain = token.find(domain[0]) + index\n",
    "          if token.find(domain[0]) == 0:\n",
    "              index_end_domain = index_start_domain + len(token)\n",
    "          else:\n",
    "              index_end_domain = index_start_domain + len(domain[0])\n",
    "          indexs.append((index_start_domain, index_end_domain))\n",
    "      index += len(token) + 1\n",
    "    return indexs\n",
    "\n",
    "def is_Email(token):\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    for word in token.split(\" \"):\n",
    "        # print(word)\n",
    "        emails = re.findall(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", word)\n",
    "        # print(emails)\n",
    "        if len(emails) != 0:\n",
    "            index_start_email = word.find(emails[0]) + index\n",
    "            \n",
    "            index_end_email = index_start_email + len(emails[0])\n",
    "            \n",
    "            indexs.append((index_start_email, index_end_email))\n",
    "        index += len(word) + 1\n",
    "    return indexs\n",
    "def is_IP(token):\n",
    "  index = 0\n",
    "  indexs = []\n",
    "  for word in token.split(\" \"):\n",
    "      # print(word)\n",
    "      emails = re.findall(r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\", word)\n",
    "      # print(emails)\n",
    "      if len(emails) != 0:\n",
    "          index_start_email = word.find(emails[0]) + index\n",
    "          \n",
    "          index_end_email = index_start_email + len(emails[0])\n",
    "          \n",
    "          indexs.append((index_start_email, index_end_email))\n",
    "      index += len(word) + 1\n",
    "  return indexs\n",
    "\n",
    "def post_process_email_url(datas):\n",
    "  black_word = [\"tp.hcm\"]\n",
    "  datas_trained = []\n",
    "  for i in range(len(datas)):\n",
    "    data = datas[i]\n",
    "\n",
    "      # check predict email\n",
    "    if data[1] == 'EMAIL':\n",
    "        check = is_Email(data[0])\n",
    "        if len(check) == 0:\n",
    "          data = (data[0], 'O')\n",
    "    \n",
    "    elif data[1] == 'URL':\n",
    "        check = is_URL(data[0])\n",
    "\n",
    "        if len(check) == 0 or  check[0][1] - check[0][0]!= len(data[0]):\n",
    "        \n",
    "          data = (data[0], 'O')\n",
    "    \n",
    "    elif data[1] == 'IP':\n",
    "        check = is_IP(data[0])\n",
    "        if len(check) == 0 or  check[0][1] - check[0][0]!= len(data[0]):\n",
    "          if data[0].isalnum():\n",
    "            data = (data[0], 'QUANTITY')\n",
    "          else:\n",
    "            data = (data[0], 'O')\n",
    "\n",
    "          # return\n",
    "    if data[1] in ['O'] and data[1].lower() not in black_word:\n",
    "        # print(data[0])\n",
    "        check_url = is_URL(data[0])\n",
    "        check_email= is_Email(data[0])\n",
    "\n",
    "        if len(check_url) > 0 and  check_url[0][1] - check_url[0][0] == len(data[0]):\n",
    "\n",
    "          data = (data[0], 'URL')\n",
    "\n",
    "        elif len(check_email) > 0:\n",
    "          data = (data[0], 'EMAIL')\n",
    "      \n",
    "    datas_trained.append(data)\n",
    "  return datas_trained"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "is_ADDRESS(\"12 Nguyễn Rri Cong, Nam tu Liem\", [(\"12 Nguyễn Rri Cong\", \"ADDRESS\"), (\"Nam tu Liem\", \"ADDRESS\")])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "'I-PHONENUMBER-MANH'.split('-', 1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I', 'PHONENUMBER-MANH']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "dff = read_conll('/Users/phamvanmanh/Desktop/machine/final_vlsp/converted/train-converted/00_add_0269.conll')\n",
    "df1 = pd.DataFrame(dff[0])\n",
    "df1[df1['label1'] == 'PHONENUMBER']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>n_sent</th>\n",
       "      <th>word</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>00_add_0269</td>\n",
       "      <td>58</td>\n",
       "      <td>0987673608</td>\n",
       "      <td>PHONENUMBER</td>\n",
       "      <td>O</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f_name  n_sent        word       label1 label2 label3\n",
       "399  00_add_0269      58  0987673608  PHONENUMBER      O      ?"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def to_zip(df, parent_path, fold_name):\n",
    "\n",
    "    compression_opts = dict(method='zip',\n",
    "                                archive_name=fold_name + '.csv')  \n",
    "    df.to_csv( parent_path + fold_name + '.zip', index=False,\n",
    "                compression=compression_opts) \n",
    "    return print(fold_name + \" saved done!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import os\n",
    "\n",
    "def convert_Dataframe(in_folder, out_folder):\n",
    "    \n",
    "    sub_folders = [name for name in os.listdir(in_folder) if os.path.isdir(os.path.join(in_folder, name))]\n",
    "    print(sub_folders)\n",
    "    for n_folder in sub_folders:\n",
    "        df = pd.DataFrame(columns = ['f_name','n_sent', 'word', 'label1', 'label2', 'label3'])\n",
    "        path = in_folder + n_folder\n",
    "        sub_files = os.listdir(path)\n",
    "    \n",
    "  \n",
    "        for s_file in sub_files:\n",
    "            try:\n",
    "                f_path = path + '/' + s_file\n",
    "                data = read_conll(f_path)\n",
    "                df1 = pd.DataFrame(data[0])\n",
    "                df = df.append(df1)\n",
    "            except  Exception as e: print('error read_conll: {}, {}'.format(e, f_path))\n",
    "\n",
    "        try:\n",
    "            to_zip(df, out_folder, n_folder)\n",
    "        except  Exception as e: print('error to_zip: {}'.format(e))\n",
    "        \n",
    "\n",
    "\n",
    "    return 'done!'\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "x"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "out_folder = '/Users/phamvanmanh/Desktop/machine/final_vlsp/converted_to_dataframe/'\n",
    "in_folder = '/Users/phamvanmanh/Desktop/machine/final_vlsp/converted/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "convert_Dataframe(in_folder, out_folder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['test-converted', 'dev-converted', 'train-converted']\n",
      "test-converted saved done!\n",
      "dev-converted saved done!\n",
      "train-converted saved done!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'done!'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('mlenv': venv)"
  },
  "interpreter": {
   "hash": "64eef7adee6260e839badb1decb60292ef8dcf6f0c9c7bd538f98ad735b6d354"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}