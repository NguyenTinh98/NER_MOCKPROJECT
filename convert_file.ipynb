{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from utils.processing_data import is_IP"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/phamvanmanh/Documents/GitHub/NER_MOCKPROJECT/test_final.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "import string\n",
    "import unicodedata\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_email_url(datas):\n",
    "  datas_trained = []\n",
    "  for i in range(len(datas)):\n",
    "    data = datas[i]\n",
    "\n",
    "    if data[1] == 'EMAIL':\n",
    "      check = is_Email(data[0])\n",
    "      if len(check) == 0:\n",
    "        data = (data[0], 'O')\n",
    "    \n",
    "    if data[1] != 'EMAIL' and  data[1] != 'URL': #(url, org, loc, o,.....)\n",
    "      check = is_Email(data[0])\n",
    "      if len(check) > 0:\n",
    "        data = (data[0], 'EMAIL')\n",
    "\n",
    "  \n",
    "\n",
    "    if data[1] == \"URL\":\n",
    "      # print(data[0])\n",
    "      check = is_URL(data[0])\n",
    "      if len(check) > 0 and  check[0][1] - check[0][0] == len(data[0]):\n",
    "        data = (data[0], 'URL')\n",
    "      else: \n",
    "        data = (data[0], 'O')\n",
    "      \n",
    "    try:\n",
    "      if data[1] != 'URL' and data[1] != 'EMAIL':\n",
    "        check = is_URL(data[0])\n",
    "        if len(check) > 0 and  check[0][1] - check[0][0] == len(data[0]):\n",
    "          data = (data[0], 'URL')\n",
    "    except:\n",
    "      print(check)\n",
    "    datas_trained.append(data)\n",
    "  return datas_trained\n",
    "    \n",
    "# sent = 'pham van manh have email ( pvm26042000@gmail.com ) ....'\n",
    "# out = [('pham', 'O'), ('van', 'O'), ('manh', 'O'), ('have', 'O'), ('email', 'O'), ('(', 'O'),  ('pvm26042000', 'EMAIL'), ('@', 'EMAIL'),('gmail', 'EMAIL'), ('.', 'EMAIL'),('com', 'EMAIL'),(')', 'O'),('....', 'O')]\n",
    "\n",
    "def merge_word(sent, pred_out):\n",
    "  '''\n",
    "    :sent: is input sentences (hanlded pre-processing). example: 'pham van manh have email ( pvm26042000@gmail.com ) ....'\n",
    "    :out : is input of predict, is list tuple. example: [('pham', 'O'), ('van', 'O'), ('manh', 'O'), ('have', 'O'), ('email', 'O'), ('(', 'O'),  ('pvm26042000', 'EMAIL'), ('@', 'EMAIL'),('gmail', 'EMAIL'), ('.', 'EMAIL'),('com', 'EMAIL'),(')', 'O'),('....', 'O')]\n",
    "  '''\n",
    "  out_merged = []\n",
    "  parts = sent.split()\n",
    "  for index in range(0, len(parts)):\n",
    "    word = parts[index]\n",
    "\n",
    "    \n",
    "    for jndex in range(1, len(pred_out) + 1):\n",
    "      token = pred_out[0:jndex]\n",
    "      ws_token, _ = list(zip(*token))\n",
    "      word_token = \"\".join(ws_token)\n",
    "   \n",
    "      if word_token == word:\n",
    "        if len(token) == 1:\n",
    "          out_merged.append(token[0])\n",
    "        elif len(token) > 1:\n",
    "          a, b = list(zip(*token))\n",
    "          word_merged = \"\".join(a)\n",
    "          l_merged = decide_label((word_merged, b))\n",
    "          out_merged.append(l_merged)\n",
    "        pred_out = pred_out[jndex:]\n",
    "        break\n",
    "  return out_merged\n",
    "\n",
    "def post_processing(origin_sentence, out_predict):\n",
    "\n",
    "  out_merged = merge_word(origin_sentence, out_predict)\n",
    "  datas_trained = post_process_email_url(out_merged)\n",
    "  \n",
    "  gr_indexs = cluster(datas_trained, 3)\n",
    "  print(gr_indexs)\n",
    "  for index in gr_indexs:\n",
    "    string, label = list(zip(*datas_trained[index[0]: index[-1] + 1]))\n",
    "\n",
    "    if is_ADDRESS(string, label) == True:\n",
    "      for i in range(index[0], index[-1] + 1):\n",
    "        datas_trained[i] =(datas_trained[i][0], \"ADDRESS\")\n",
    "    else:\n",
    "      for i in range(index[0], index[-1] + 1):\n",
    "        if datas_trained[i][0] == ',':\n",
    "          datas_trained[i] = (datas_trained[i][0], \"O\")\n",
    "        else:\n",
    "          datas_trained[i] =(datas_trained[i][0], \"LOCATION\")\n",
    "  return datas_trained\n",
    "\n",
    "def cluster(data, maxgap):\n",
    "    '''Arrange data into groups where successive elements\n",
    "       differ by no more than *maxgap*\n",
    "        >>> cluster([1, 6, 9, 100, 102, 105, 109, 134, 139], maxgap=10)\n",
    "        [[1, 6, 9], [100, 102, 105, 109], [134, 139]]\n",
    "        >>> cluster([1, 6, 9, 99, 100, 102, 105, 134, 139, 141], maxgap=10)\n",
    "        [[1, 6, 9], [99, 100, 102, 105], [134, 139, 141]]\n",
    "    '''\n",
    "\n",
    "    black_list = [\":\", \"(\", \";\", \"{\", \"[\"]\n",
    "\n",
    "    indexs = []\n",
    "    for index in range(len(data)):\n",
    "      token = data[index]\n",
    "      if token[1] == \"LOCATION\" or token[1] == \"ADDRESS\" :\n",
    "        indexs.append(index)\n",
    "\n",
    "    indexs.sort()\n",
    "    groups = [[indexs[0]]]\n",
    "\n",
    "    for jndex in range(1,len(indexs[1:])):\n",
    "      x  = indexs[jndex]\n",
    "      # print(data[indexs[jndex-1]:x])\n",
    "      w, labels = list(zip(*data[indexs[jndex-1]:x]))\n",
    "      # print(any(character in w for character in black_list))\n",
    "      if abs(x - groups[-1][-1]) <= maxgap and any(character in w for character in black_list) == False:\n",
    "          groups[-1].append(x)\n",
    "      elif any(character in data[indexs[jndex-1]:x] for character in black_list):\n",
    "          groups.append([x])\n",
    "      else:\n",
    "          groups.append([x])\n",
    "    return groups\n",
    "  \n",
    "\n",
    "\n",
    "def has_numbers(inputString):\n",
    "  parts = inputString.split()\n",
    "  for i in range(len(parts)):\n",
    "    part = parts[i]\n",
    "    for char in part:\n",
    "      if char.isdigit():\n",
    "        if i > 0 and parts[i-1].lower() in [\"quận\", \"q.\"]:\n",
    "          return False\n",
    "        else:\n",
    "          return True\n",
    "  return False\n",
    "\n",
    "def is_ADDRESS(string, label):\n",
    "\n",
    "  uy_tin = 0\n",
    "  string_loc = \" \".join(string)\n",
    "\n",
    "  level = [\"số\", \"lô\", \"km\",\"quốc_lộ\",\"đại_lộ\",\"kcn\", \"đường\",\"tổ\", \"ngõ\", \"toà\", \"ngách\", \"hẻm\",\"kiệt\", \"chung_cư\", \"số_nhà\",\"ấp\" ,\"thôn\", \"khu\",\"phố\" , \"quận\", \"phường\", \"xã\", \"thị_xã\",\"huyện\", \"thành_phố\", \"tp\", \"tỉnh\" ]\n",
    "  level_0 ={'status': True,'keywords': [\"toà\", \"chung_cư\", \"số\", \"lô\", \"số_nhà\"] }\n",
    "  level_1 = {'status': True, 'keywords': [ \"ngõ\", \"ngách\", \"hẻm\",\"kiệt\",\"kcn\", \"km\"]}\n",
    "  level_2 = {'status': True, 'keywords':[\"ấp\" ,\"thôn\", \"khu\",\"phố\" , \"quận\", \"phường\", \"xã\", \"tổ\", \"dân_phố\", \"đường\", \"quốc_lộ\", \"đại_lộ\"]}\n",
    "  level_3 = {'status': True,'keywords':[\"thị\",\"huyện\"]}\n",
    "  level_4 = {'status': True,'keywords':[\"thành_phố\", \"tp\", \"tỉnh\"]}\n",
    "\n",
    "  parts =  ViPosTagger.postagging(ViTokenizer.tokenize(string_loc))[0]\n",
    "\n",
    "  for index in range(len(parts)):\n",
    "    seg_word = parts[index]\n",
    "    if index == 0 and  has_numbers(seg_word.split(\" \")[0]):\n",
    "        uy_tin += 0.3\n",
    "        break\n",
    "\n",
    "    if seg_word.lower() in level:\n",
    " \n",
    "      if seg_word.lower() in level_0['keywords'] and level_0['status'] == True:\n",
    "        uy_tin += 0.3\n",
    "        break\n",
    "\n",
    "      elif seg_word.lower() in level_1['keywords'] and level_1['status'] == True:\n",
    "        uy_tin += 0.25\n",
    "        level_1['status'] = False\n",
    "\n",
    "      elif seg_word.lower()  in level_2['keywords'] and level_2['status'] == True:\n",
    "        uy_tin += 0.025\n",
    "        level_2['status'] = False\n",
    "      elif seg_word.lower() in  level_3['keywords'] and level_3['status'] == True:\n",
    "   \n",
    "        uy_tin += 0.015\n",
    "        level_3['status'] = False\n",
    "      elif seg_word.lower() in level_4['keywords'] and level_4['status'] == True:\n",
    "     \n",
    "        uy_tin += 0.01\n",
    "        level_4['status'] = False\n",
    "  print(uy_tin)\n",
    "  if uy_tin >= 0.29:\n",
    "    return True\n",
    "  \n",
    "  return False\n",
    "\n",
    "\n",
    "def decide_label(part):\n",
    "  word = part[0]\n",
    "  labels = part[1]\n",
    "  return (word, max(labels))\n",
    "\n",
    "\n",
    "import re\n",
    "def constain_alpha(token):\n",
    "\n",
    "  for character in token:\n",
    "\n",
    "    is_letter = character.isalpha()\n",
    "    if is_letter == True:\n",
    "      return True\n",
    "  \n",
    "  return False\n",
    "\n",
    "def is_URL(token):\n",
    "    token = token.lower()\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    if constain_alpha(token) == True:\n",
    "      domain = re.findall(r'\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b', token)\n",
    "      \n",
    "      if len(domain) != 0:\n",
    "          index_start_domain = token.find(domain[0]) + index\n",
    "          if token.find(domain[0]) == 0:\n",
    "              index_end_domain = index_start_domain + len(token)\n",
    "          else:\n",
    "              index_end_domain = index_start_domain + len(domain[0])\n",
    "          indexs.append((index_start_domain, index_end_domain))\n",
    "      index += len(token) + 1\n",
    "    return indexs\n",
    "\n",
    "def is_Email(token):\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    for word in token.split(\" \"):\n",
    "        # print(word)\n",
    "        emails = re.findall(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", word)\n",
    "        # print(emails)\n",
    "        if len(emails) != 0:\n",
    "            index_start_email = word.find(emails[0]) + index\n",
    "            \n",
    "            index_end_email = index_start_email + len(emails[0])\n",
    "            \n",
    "            indexs.append((index_start_email, index_end_email))\n",
    "        index += len(word) + 1\n",
    "    return indexs\n",
    "def is_IP(token):\n",
    "  index = 0\n",
    "  indexs = []\n",
    "  for word in token.split(\" \"):\n",
    "      # print(word)\n",
    "      emails = re.findall(r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\", word)\n",
    "      # print(emails)\n",
    "      if len(emails) != 0:\n",
    "          index_start_email = word.find(emails[0]) + index\n",
    "          \n",
    "          index_end_email = index_start_email + len(emails[0])\n",
    "          \n",
    "          indexs.append((index_start_email, index_end_email))\n",
    "      index += len(word) + 1\n",
    "  return indexs\n",
    "\n",
    "def post_process_email_url(datas):\n",
    "  black_word = [\"tp.hcm\"]\n",
    "  datas_trained = []\n",
    "  for i in range(len(datas)):\n",
    "    data = datas[i]\n",
    "\n",
    "      # check predict email\n",
    "    if data[1] == 'EMAIL':\n",
    "        check = is_Email(data[0])\n",
    "        if len(check) == 0:\n",
    "          data = (data[0], 'O')\n",
    "    \n",
    "    elif data[1] == 'URL':\n",
    "        check = is_URL(data[0])\n",
    "        if len(check) == 0 or  check[0][1] - check[0][0]!= len(data[0]):\n",
    "          data = (data[0], 'O')\n",
    "    \n",
    "    elif data[1] == 'IP':\n",
    "        check = is_IP(data[0])\n",
    "        if len(check) == 0 or  check[0][1] - check[0][0]!= len(data[0]):\n",
    "          if data[0].isalnum():\n",
    "            data = (data[0], 'QUANTITY')\n",
    "          else:\n",
    "            data = (data[0], 'O')\n",
    "\n",
    "    if data[1] in ['O'] and data[1].lower() not in black_word:\n",
    "        # print(data[0])\n",
    "        check_url = is_URL(data[0])\n",
    "        check_email= is_Email(data[0])\n",
    "\n",
    "        if len(check_url) > 0 and  check_url[0][1] - check_url[0][0] == len(data[0]):\n",
    "\n",
    "          data = (data[0], 'URL')\n",
    "\n",
    "        elif len(check_email) > 0:\n",
    "          data = (data[0], 'EMAIL')\n",
    "      \n",
    "    datas_trained.append(data)\n",
    "  return datas_trained"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "data_add = []\n",
    "for d in data:\n",
    "    w, tr_n, pred_n, pos_n = list(zip(*d))\n",
    "    if \"ADDRESS\" in tr_n:\n",
    "        for index in range(len(tr_n)):\n",
    "            if tr_n[index] != pos_n[index] and pos_n[index] == \"ADDRESS\":\n",
    "                data_add.append(d)\n",
    "                break\n",
    "len(data_add)  \n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "data_add[4]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Chung', 'O', 'O', 'O'),\n",
       " ('cư', 'O', 'O', 'O'),\n",
       " ('cao', 'O', 'O', 'O'),\n",
       " ('nhất', 'O', 'O', 'O'),\n",
       " (':', 'O', 'O', 'O'),\n",
       " ('Năm', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " ('2014', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " (',', 'O', 'O', 'O'),\n",
       " ('với', 'O', 'O', 'O'),\n",
       " ('việc', 'O', 'O', 'O'),\n",
       " ('sở', 'O', 'O', 'O'),\n",
       " ('hữu', 'O', 'O', 'O'),\n",
       " ('tòa', 'O', 'O', 'O'),\n",
       " ('nhà', 'O', 'O', 'O'),\n",
       " ('số', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('432', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('đại', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('lộ', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('Park', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('Avenue', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " (',', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('thành', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('phố', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('New', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('York', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('(', 'O', 'O', 'ADDRESS'),\n",
       " ('Mỹ', 'LOCATION', 'LOCATION', 'ADDRESS'),\n",
       " (')', 'O', 'O', 'O'),\n",
       " ('vượt', 'O', 'O', 'O'),\n",
       " ('qua', 'O', 'O', 'O'),\n",
       " ('kỷ', 'O', 'O', 'O'),\n",
       " ('lục', 'O', 'O', 'O'),\n",
       " ('tòa', 'O', 'O', 'O'),\n",
       " ('chung', 'O', 'O', 'O'),\n",
       " ('cư', 'O', 'O', 'O'),\n",
       " ('cao', 'O', 'O', 'O'),\n",
       " ('nhất', 'O', 'O', 'O'),\n",
       " ('thế', 'O', 'O', 'O'),\n",
       " ('giới', 'O', 'O', 'O'),\n",
       " ('của', 'O', 'O', 'O'),\n",
       " ('Dubai', 'LOCATION', 'LOCATION', 'LOCATION'),\n",
       " ('.', 'O', 'O', 'O'),\n",
       " ('Khoảng', 'O', 'O', 'O'),\n",
       " ('cách', 'O', 'O', 'O'),\n",
       " ('giữa', 'O', 'O', 'O'),\n",
       " ('nóc', 'O', 'O', 'O'),\n",
       " ('chung', 'O', 'O', 'O'),\n",
       " ('cư', 'O', 'O', 'O'),\n",
       " ('và', 'O', 'O', 'O'),\n",
       " ('mặt', 'O', 'O', 'O'),\n",
       " ('đất', 'O', 'O', 'O'),\n",
       " ('là', 'O', 'O', 'O'),\n",
       " ('hơn', 'O', 'O', 'O'),\n",
       " ('426', 'QUANTITY', 'QUANTITY', 'QUANTITY'),\n",
       " ('m', 'QUANTITY', 'QUANTITY', 'QUANTITY'),\n",
       " ('.', 'O', 'O', 'O'),\n",
       " ('(', 'O', 'O', 'O'),\n",
       " ('Nguồn', 'O', 'O', 'O'),\n",
       " (':', 'O', 'O', 'O'),\n",
       " ('Getty', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " ('Images', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " (')', 'O', 'O', 'O')]"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "w, true, pred, pos_ner = list(zip(*data_add[3]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "pred_out = list(zip(w, pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "post_processing(\" \".join(w), pred_out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [15, 16, 17, 18, 19, 20, 21, 22, 23], [25, 26, 27, 28, 29, 30, 31, 32, 33]]\n",
      "0.3\n",
      "0.3\n",
      "0.3\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Ngoài', 'O'),\n",
       " ('ra', 'O'),\n",
       " (',', 'O'),\n",
       " ('tại', 'O'),\n",
       " ('463', 'ADDRESS'),\n",
       " ('Trần', 'ADDRESS'),\n",
       " ('Hưng', 'ADDRESS'),\n",
       " ('Đạo', 'ADDRESS'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Dĩ', 'ADDRESS'),\n",
       " ('An', 'ADDRESS'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Bình', 'ADDRESS'),\n",
       " ('Dương', 'ADDRESS'),\n",
       " (';', 'O'),\n",
       " ('63', 'ADDRESS'),\n",
       " ('Hoàng', 'ADDRESS'),\n",
       " ('Cầu', 'ADDRESS'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Đống', 'ADDRESS'),\n",
       " ('Đa', 'ADDRESS'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Hà', 'ADDRESS'),\n",
       " ('Nội', 'ADDRESS'),\n",
       " (';', 'O'),\n",
       " ('24', 'ADDRESS'),\n",
       " ('Lý', 'ADDRESS'),\n",
       " ('Thường', 'ADDRESS'),\n",
       " ('Kiệt', 'ADDRESS'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Huế', 'ADDRESS'),\n",
       " (',', 'ADDRESS'),\n",
       " ('Thừa', 'ADDRESS'),\n",
       " ('Thiên', 'ADDRESS'),\n",
       " ('Huế', 'ADDRESS'),\n",
       " (';', 'O'),\n",
       " ('Trung', 'ORGANIZATION'),\n",
       " ('tâm', 'ORGANIZATION'),\n",
       " ('Giao', 'ORGANIZATION'),\n",
       " ('lưu', 'ORGANIZATION'),\n",
       " ('văn', 'ORGANIZATION'),\n",
       " ('hóa', 'ORGANIZATION'),\n",
       " ('phố', 'ORGANIZATION'),\n",
       " ('cổ', 'ORGANIZATION'),\n",
       " ('Hà', 'ORGANIZATION'),\n",
       " ('Nội', 'ORGANIZATION'),\n",
       " ('tổ', 'O'),\n",
       " ('chức', 'O'),\n",
       " ('trưng', 'O'),\n",
       " ('bày', 'O'),\n",
       " ('tư', 'O'),\n",
       " ('liệu', 'O'),\n",
       " (',', 'O'),\n",
       " ('hình', 'O'),\n",
       " ('ảnh', 'O'),\n",
       " ('với', 'O'),\n",
       " ('chủ', 'O'),\n",
       " ('đề', 'O'),\n",
       " ('\"', 'O'),\n",
       " ('Tết', 'EVENT'),\n",
       " ('Trung', 'EVENT'),\n",
       " ('thu', 'EVENT'),\n",
       " ('xưa', 'EVENT'),\n",
       " ('\"', 'O'),\n",
       " (',', 'O'),\n",
       " ('hướng', 'O'),\n",
       " ('dẫn', 'O'),\n",
       " ('làm', 'O'),\n",
       " ('đồ', 'O'),\n",
       " ('chơi', 'O'),\n",
       " ('truyền', 'O'),\n",
       " ('thống', 'O'),\n",
       " (',', 'O'),\n",
       " ('biểu', 'O'),\n",
       " ('diễn', 'O'),\n",
       " ('trống', 'PRODUCT'),\n",
       " ('Đọi', 'PRODUCT'),\n",
       " ('Tam', 'PRODUCT'),\n",
       " ('...', 'O')]"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "a = [1,2,3]\n",
    "b = [1,2,5]\n",
    "a ==b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# 6, 15 # dev\n",
    "# 8, 10\n",
    "len(data_add)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "data_add[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Nhân', 'O', 'O', 'O'),\n",
       " ('dịp', 'O', 'O', 'O'),\n",
       " ('này', 'O', 'O', 'O'),\n",
       " (',', 'O', 'O', 'O'),\n",
       " ('Fahasa', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " ('đã', 'O', 'O', 'O'),\n",
       " ('tổ', 'O', 'O', 'O'),\n",
       " ('chức', 'O', 'O', 'O'),\n",
       " ('\"', 'O', 'O', 'O'),\n",
       " ('Tuần', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('lễ', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('triển', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('lãm', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('sách', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('Nhật', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('Bản', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('tiếng', 'EVENT', 'MISCELLANEOUS', 'MISCELLANEOUS'),\n",
       " ('Nhật', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('và', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('khai', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('trương', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('gian', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('hàng', 'EVENT', 'EVENT', 'EVENT'),\n",
       " ('\"', 'O', 'O', 'O'),\n",
       " ('với', 'O', 'O', 'O'),\n",
       " ('10.000', 'QUANTITY', 'QUANTITY', 'QUANTITY'),\n",
       " ('tên', 'O', 'O', 'O'),\n",
       " ('sách', 'O', 'O', 'O'),\n",
       " ('tại', 'O', 'O', 'O'),\n",
       " ('gian', 'O', 'O', 'O'),\n",
       " ('hàng', 'O', 'O', 'O'),\n",
       " ('sách', 'O', 'O', 'O'),\n",
       " ('tiếng', 'MISCELLANEOUS', 'MISCELLANEOUS', 'MISCELLANEOUS'),\n",
       " ('Nhật', 'MISCELLANEOUS', 'MISCELLANEOUS', 'MISCELLANEOUS'),\n",
       " ('.', 'O', 'O', 'O'),\n",
       " ('Tất', 'O', 'O', 'O'),\n",
       " ('cả', 'O', 'O', 'O'),\n",
       " ('đều', 'O', 'O', 'O'),\n",
       " ('là', 'O', 'O', 'O'),\n",
       " ('những', 'O', 'O', 'O'),\n",
       " ('cuốn', 'O', 'O', 'O'),\n",
       " ('sách', 'O', 'O', 'O'),\n",
       " ('được', 'O', 'O', 'O'),\n",
       " ('Fahasa', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " ('tuyển', 'O', 'O', 'O'),\n",
       " ('chọn', 'O', 'O', 'O'),\n",
       " ('với', 'O', 'O', 'O'),\n",
       " ('tổng', 'O', 'O', 'O'),\n",
       " ('giá', 'O', 'O', 'O'),\n",
       " ('trị', 'O', 'O', 'O'),\n",
       " ('hàng', 'O', 'O', 'O'),\n",
       " ('hóa', 'O', 'O', 'O'),\n",
       " ('trưng', 'O', 'O', 'O'),\n",
       " ('bày', 'O', 'O', 'O'),\n",
       " ('trong', 'O', 'O', 'O'),\n",
       " ('đợt', 'O', 'O', 'O'),\n",
       " ('này', 'O', 'O', 'O'),\n",
       " ('là', 'O', 'O', 'O'),\n",
       " ('gần', 'O', 'O', 'O'),\n",
       " ('7', 'QUANTITY', 'QUANTITY', 'QUANTITY'),\n",
       " ('tỷ', 'QUANTITY', 'QUANTITY', 'QUANTITY'),\n",
       " ('đồng', 'QUANTITY', 'QUANTITY', 'QUANTITY'),\n",
       " (',', 'O', 'O', 'O'),\n",
       " ('để', 'O', 'O', 'O'),\n",
       " ('phục', 'O', 'O', 'O'),\n",
       " ('vụ', 'O', 'O', 'O'),\n",
       " ('nhu', 'O', 'O', 'O'),\n",
       " ('cầu', 'O', 'O', 'O'),\n",
       " ('học', 'O', 'O', 'O'),\n",
       " ('tập', 'O', 'O', 'O'),\n",
       " ('và', 'O', 'O', 'O'),\n",
       " ('nghiên', 'O', 'O', 'O'),\n",
       " ('cứu', 'O', 'O', 'O'),\n",
       " ('bằng', 'O', 'O', 'O'),\n",
       " ('tiếng', 'MISCELLANEOUS', 'MISCELLANEOUS', 'MISCELLANEOUS'),\n",
       " ('Nhật', 'MISCELLANEOUS', 'MISCELLANEOUS', 'MISCELLANEOUS'),\n",
       " ('cho', 'O', 'O', 'O'),\n",
       " ('bạn', 'O', 'O', 'O'),\n",
       " ('đọc', 'O', 'O', 'O'),\n",
       " ('Việt', 'LOCATION', 'LOCATION', 'LOCATION'),\n",
       " ('Nam', 'LOCATION', 'LOCATION', 'LOCATION'),\n",
       " ('cũng', 'O', 'O', 'O'),\n",
       " ('như', 'O', 'O', 'O'),\n",
       " ('các', 'O', 'O', 'O'),\n",
       " ('gia', 'O', 'O', 'O'),\n",
       " ('đình', 'O', 'O', 'O'),\n",
       " ('người', 'MISCELLANEOUS', 'MISCELLANEOUS', 'MISCELLANEOUS'),\n",
       " ('Nhật', 'MISCELLANEOUS', 'MISCELLANEOUS', 'MISCELLANEOUS'),\n",
       " ('sinh', 'O', 'O', 'O'),\n",
       " ('sống', 'O', 'O', 'O'),\n",
       " ('và', 'O', 'O', 'O'),\n",
       " ('làm', 'O', 'O', 'O'),\n",
       " ('việc', 'O', 'O', 'O'),\n",
       " ('tại', 'O', 'O', 'O'),\n",
       " ('Hà', 'LOCATION', 'LOCATION', 'LOCATION'),\n",
       " ('Nội', 'LOCATION', 'LOCATION', 'LOCATION'),\n",
       " ('và', 'O', 'O', 'O'),\n",
       " ('các', 'O', 'O', 'O'),\n",
       " ('tỉnh', 'O', 'O', 'O'),\n",
       " ('lân', 'O', 'O', 'O'),\n",
       " ('cận', 'O', 'O', 'O'),\n",
       " ('.', 'O', 'O', 'O'),\n",
       " ('Chương', 'O', 'O', 'O'),\n",
       " ('trình', 'O', 'O', 'O'),\n",
       " ('diễn', 'O', 'O', 'O'),\n",
       " ('ra', 'O', 'O', 'O'),\n",
       " ('từ', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " ('21', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " ('đến', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " ('28-9', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " ('tại', 'O', 'O', 'O'),\n",
       " ('TTTM', 'LOCATION', 'LOCATION', 'ADDRESS'),\n",
       " ('Aeon', 'LOCATION', 'LOCATION', 'ADDRESS'),\n",
       " ('Long', 'LOCATION', 'LOCATION', 'ADDRESS'),\n",
       " ('Biên', 'LOCATION', 'LOCATION', 'ADDRESS'),\n",
       " (',', 'O', 'O', 'O'),\n",
       " ('số', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('27', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('đường', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('Cổ', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('Linh', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('.', 'O', 'O', 'O')]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "data_add[11]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Theo', 'O', 'O', 'O'),\n",
       " ('tạp', 'O', 'O', 'O'),\n",
       " ('chí', 'O', 'O', 'O'),\n",
       " ('Vanity', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " ('Fair', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " (',', 'O', 'O', 'O'),\n",
       " ('hôm', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " ('2/10/2017', 'DATETIME', 'DATETIME', 'DATETIME'),\n",
       " (',', 'O', 'O', 'O'),\n",
       " ('Harvey', 'PERSON', 'PERSON', 'PERSON'),\n",
       " ('Weinstein', 'PERSON', 'PERSON', 'PERSON'),\n",
       " ('đi', 'O', 'O', 'O'),\n",
       " ('làm', 'O', 'O', 'O'),\n",
       " ('sớm', 'O', 'O', 'O'),\n",
       " ('hơn', 'O', 'O', 'O'),\n",
       " ('thường', 'O', 'O', 'O'),\n",
       " ('lệ', 'O', 'O', 'O'),\n",
       " ('.', 'O', 'O', 'O'),\n",
       " ('Như', 'O', 'O', 'O'),\n",
       " ('mọi', 'O', 'O', 'O'),\n",
       " ('ngày', 'O', 'O', 'O'),\n",
       " (',', 'O', 'O', 'O'),\n",
       " ('ông', 'O', 'O', 'O'),\n",
       " ('ta', 'O', 'O', 'O'),\n",
       " ('quát', 'O', 'O', 'O'),\n",
       " ('tháo', 'O', 'O', 'O'),\n",
       " ('nhân', 'O', 'O', 'O'),\n",
       " ('viên', 'O', 'O', 'O'),\n",
       " ('ầm', 'O', 'O', 'O'),\n",
       " ('ĩ', 'O', 'O', 'O'),\n",
       " ('trong', 'O', 'O', 'O'),\n",
       " ('văn', 'O', 'O', 'O'),\n",
       " ('phòng', 'O', 'O', 'O'),\n",
       " ('The', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " ('Weinstein', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " ('Company', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " ('(', 'O', 'O', 'O'),\n",
       " ('TWC', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION'),\n",
       " (')', 'O', 'O', 'O'),\n",
       " ('tại', 'O', 'O', 'O'),\n",
       " ('địa', 'O', 'O', 'O'),\n",
       " ('chỉ', 'O', 'O', 'O'),\n",
       " ('375', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('Greenwich', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('Street', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " (',', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('New', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('York', 'ADDRESS', 'ADDRESS', 'ADDRESS'),\n",
       " ('.', 'O', 'O', 'O'),\n",
       " ('Tòa', 'O', 'O', 'O'),\n",
       " ('nhà', 'O', 'O', 'O'),\n",
       " ('được', 'O', 'O', 'O'),\n",
       " ('xây', 'O', 'O', 'O'),\n",
       " ('bằng', 'O', 'O', 'O'),\n",
       " ('gạch', 'O', 'O', 'O'),\n",
       " ('đỏ', 'O', 'O', 'O'),\n",
       " ('được', 'O', 'O', 'O'),\n",
       " ('đánh', 'O', 'O', 'O'),\n",
       " ('giá', 'O', 'O', 'O'),\n",
       " ('là', 'O', 'O', 'O'),\n",
       " ('một', 'O', 'O', 'O'),\n",
       " (\"'\", 'O', 'O', 'O'),\n",
       " ('trung', 'O', 'O', 'O'),\n",
       " ('tâm', 'O', 'O', 'O'),\n",
       " ('quyền', 'O', 'O', 'O'),\n",
       " ('lực', 'O', 'O', 'O'),\n",
       " (\"'\", 'O', 'O', 'O'),\n",
       " ('của', 'O', 'O', 'O'),\n",
       " ('thế', 'O', 'O', 'O'),\n",
       " ('giới', 'O', 'O', 'O'),\n",
       " ('điện', 'O', 'O', 'O'),\n",
       " ('ảnh', 'O', 'O', 'O'),\n",
       " ('Mỹ', 'LOCATION', 'LOCATION', 'LOCATION'),\n",
       " ('.', 'O', 'O', 'O')]"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "w, tr_n, pred_n, pos_n = list(zip(*data[2]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\" \".join(w)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Liên quan đến việc khắc phục hậu quả trong tội cố ý làm trái , dẫn Điều 193 của BLHS , luật sư Hoàng Anh Tuấn nêu quan điểm , trong trường hợp này Vũ Đức Thuận là người nhân danh pháp nhân ( PVC ) thực hiện các giao dịch vì lợi ích của pháp nhân , tức việc chi tiêu tiền là có lợi cho PVC . Với tư cách đó , pháp nhân chịu trách nhiệm với thiệt hại mà người đại diện thực hiện vì pháp nhân gây ra .'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def to_zip(df, parent_path, fold_name):\n",
    "\n",
    "    compression_opts = dict(method='zip',\n",
    "                                archive_name=fold_name + '.csv')  \n",
    "    df.to_csv( parent_path + fold_name + '.zip', index=False,\n",
    "                compression=compression_opts) \n",
    "    return print(fold_name + \" saved done!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import os\n",
    "\n",
    "def convert_Dataframe(in_folder, out_folder):\n",
    "    \n",
    "    sub_folders = [name for name in os.listdir(in_folder) if os.path.isdir(os.path.join(in_folder, name))]\n",
    "    print(sub_folders)\n",
    "    for n_folder in sub_folders:\n",
    "        df = pd.DataFrame(columns = ['f_name','n_sent', 'word', 'label1', 'label2', 'label3'])\n",
    "        path = in_folder + n_folder\n",
    "        sub_files = os.listdir(path)\n",
    "    \n",
    "  \n",
    "        for s_file in sub_files:\n",
    "            try:\n",
    "                f_path = path + '/' + s_file\n",
    "                data = read_conll(f_path)\n",
    "                df1 = pd.DataFrame(data[0])\n",
    "                df = df.append(df1)\n",
    "            except  Exception as e: print('error read_conll: {}, {}'.format(e, f_path))\n",
    "\n",
    "        try:\n",
    "            to_zip(df, out_folder, n_folder)\n",
    "        except  Exception as e: print('error to_zip: {}'.format(e))\n",
    "        \n",
    "\n",
    "\n",
    "    return 'done!'\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "x"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "out_folder = '/Users/phamvanmanh/Desktop/machine/final_vlsp/converted_to_dataframe/'\n",
    "in_folder = '/Users/phamvanmanh/Desktop/machine/final_vlsp/converted/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "convert_Dataframe(in_folder, out_folder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['test-converted', 'dev-converted', 'train-converted']\n",
      "test-converted saved done!\n",
      "dev-converted saved done!\n",
      "train-converted saved done!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'done!'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('mlenv': venv)"
  },
  "interpreter": {
   "hash": "64eef7adee6260e839badb1decb60292ef8dcf6f0c9c7bd538f98ad735b6d354"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}