{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T05:30:29.652381Z","iopub.execute_input":"2021-12-15T05:30:29.653049Z","iopub.status.idle":"2021-12-15T05:30:29.670441Z","shell.execute_reply.started":"2021-12-15T05:30:29.652958Z","shell.execute_reply":"2021-12-15T05:30:29.669615Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:29.671892Z","iopub.execute_input":"2021-12-15T05:30:29.672165Z","iopub.status.idle":"2021-12-15T05:30:29.678132Z","shell.execute_reply.started":"2021-12-15T05:30:29.672128Z","shell.execute_reply":"2021-12-15T05:30:29.677475Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport torch\nfrom sklearn.metrics import classification_report\n\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertConfig, BertModel\nfrom transformers import XLMRobertaTokenizer, XLMRobertaConfig, XLMRobertaModel\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import BertForTokenClassification, AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm, trange\nimport numpy as np \n\nimport torch.nn as nn\n\n\n\nimport torch.nn.functional as F\nlog_soft = F.log_softmax","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:29.679528Z","iopub.execute_input":"2021-12-15T05:30:29.680587Z","iopub.status.idle":"2021-12-15T05:30:32.081848Z","shell.execute_reply.started":"2021-12-15T05:30:29.680555Z","shell.execute_reply":"2021-12-15T05:30:32.081060Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Check GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\nprint(device)\n\nMAX_LEN = 256\nBS = 32\n\nlinear_dropout = 0.3\nbert_att_dropout = 0.2\nbert_hidd_dropout = 0.2\n\nbert_lr = 1e-5\nbert_weight_decay = 1e-5\nsoftmax_lr = 5e-4\nsoftmax_weight_decay = 1e-3","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:32.083801Z","iopub.execute_input":"2021-12-15T05:30:32.084149Z","iopub.status.idle":"2021-12-15T05:30:32.126400Z","shell.execute_reply.started":"2021-12-15T05:30:32.084111Z","shell.execute_reply":"2021-12-15T05:30:32.125073Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Preprocesing data","metadata":{}},{"cell_type":"code","source":"class BertDataLoader:\n    def __init__(self, dir, tokenizer, tag_values, device, is_train = False, bs = 32, maxlen = 256):\n        self.dir_train = dir\n        self.MAX_LEN = maxlen\n        self.BATCH_SIZE = bs\n        self.tag_values = tag_values\n        self.tag2idx = {t: i for i, t in enumerate(self.tag_values)}\n        self.device = device\n        self.tokenizer = tokenizer\n        self.is_train = is_train\n    \n    def read_dataset(self):\n        with open(self.dir_train ,'rb') as f:\n            data = pickle.load(f)\n        data = [sq for sq in data if len(sq) >= 1]\n        return data\n\n    def split_data(self, data):\n        #(x,y)=> X= [x...] , Y= [y....]\n        X, Y = [], []\n        for sent in data:\n            temp_x = []\n            temp_y = []\n            for word in sent:\n                temp_x.append(word[0])\n                temp_y.append(word[1])\n            X.append(temp_x)\n            Y.append(temp_y)\n        return X, Y    \n\n    def check_label(self, data):\n        '''\n        input: [[('Hello','O'),...],...]\n        output: {'O','LOC',\"ORG\",...}\n        '''\n        a = []\n        for i in data:\n            for j in i:\n                _, l = j\n                a.append(l)\n        return list(set(a))\n\n#     def isSubword(self, x, idx, sub = '##'):\n#         return sub not in x[idx] and idx > 0 and idx < len(x) - 1 and sub not in x[idx-1] and sub not in x[idx+1]\n\n    def isNotSubword(self, x, idx, sub = '##'):\n        if sub == '##':\n            return sub not in x[idx] and idx < len(x) - 1 and sub not in x[idx+1]\n        elif sub == '@@':\n            return sub not in x[idx] and idx > 0 and sub not in x[idx-1]\n        return sub in x[idx] and idx < len(x) - 1 and sub in x[idx+1]\n    \n    def cutting_subword(self, X, y):\n        res_X, res_y = [], []\n        punct = '.!?'\n        st = 0\n        cur = 0\n\n        while (st < len(X)-self.MAX_LEN):\n            flag = True\n            for i in range(st+self.MAX_LEN-1, st-1, -1):\n                if X[i] in punct and y[i] == 'O':\n                    cur = i+1\n                    flag = False\n                    break\n            if flag:\n                for i in range(st+self.MAX_LEN-1, st-1, -1):\n                    if self.isNotSubword(X, i, sub='_'):\n                        cur = i+1\n                        if y[i] == 'O':\n                            cur = i+1\n                            break\n            if st == cur:\n                cur += self.MAX_LEN\n\n            res_X.append(X[st: cur])\n            res_y.append(y[st: cur])\n            st = cur\n\n        res_X.append(X[cur:])\n        res_y.append(y[cur:])\n        return res_X, res_y\n    \n    def add_subword(self, sentence, text_labels):\n        '''\n        input:\n            sentence = ['Phạm', 'Văn', 'Mạnh']\n            text_labels = ['B-PER', 'I-PER','I-PER']\n\n        output: \n            ['Phạm', 'Văn', 'M', '##ạnh'],\n            ['B-PER', 'I-PER', 'I-PER', 'I-PER']\n        '''\n        tokenized_sentence = []\n        labels = []\n        for word, label in zip(sentence, text_labels):\n            subwords = self.tokenizer.tokenize(word)\n            tokenized_sentence.extend(subwords)\n            \n            labels.extend([label] * len(subwords))\n        return tokenized_sentence, labels\n\n\n    def add_subword2data(self, X, Y):\n        '''\n            input:\n                sentence = [['Phạm', 'Văn', 'Mạnh',..],....]\n                text_labels = [['B-PER', 'I-PER','I-PER',..],...]\n\n            output: \n                [['Phạm', 'Văn', 'M', '##ạnh',..],....],\n                [['B-PER', 'I-PER','I-PER','I-PER',..],...]\n        '''\n        tokenized_texts_and_labels = [self.add_subword(sentence, text_labels) for sentence, text_labels in zip(X, Y)]\n        tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n        labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n        return tokenized_texts,labels\n    \n    \n    def padding_data(self,X_subword,y_subword):\n        '''\n            input:\n                X = [['Phạm', 'Văn', 'M', '##ạnh',..],....]\n                Y = [['B-PER', 'I-PER','I-PER','I-PER',..],...]\n\n            output: \n            [[10,20,30,40,0,0,0,0,0,0,0,0...],...],\n            [[1, 2,3,4,5,5,5,5,5,5,5,5,5,...],...]\n        '''\n        X_padding = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in X_subword],\n                          maxlen=self.MAX_LEN, dtype=\"long\", value=0.0,\n                          truncating=\"post\", padding=\"post\")\n\n        y_padding = pad_sequences([[self.tag2idx.get(l) for l in lab] for lab in y_subword],\n                        maxlen=self.MAX_LEN, value=self.tag2idx[\"PAD\"], padding=\"post\",\n                        dtype=\"long\", truncating=\"post\")\n        attention_masks = [[float(i != 0.0) for i in ii] for ii in X_padding]\n        \n        return X_padding, y_padding,attention_masks\n    \n    \n    def covert2tensor(self, X_padding, Y_padding, attention_masks):\n        if self.is_train == True:\n            X_tensor = torch.tensor(X_padding).to(self.device) \n            y_tensor = torch.tensor(Y_padding).to(self.device) \n            masks = torch.tensor(attention_masks).to(self.device)  \n        elif self.is_train == False:\n            X_tensor = torch.tensor(X_padding).type(torch.LongTensor).to(self.device) \n            y_tensor = torch.tensor(Y_padding).type(torch.LongTensor).to(self.device) \n            masks = torch.tensor(attention_masks).type(torch.LongTensor).to(self.device) \n        return  X_tensor, y_tensor, masks\n\n    def create_dataloader(self):\n        dataset = self.read_dataset()\n        labels = self.check_label(dataset)\n        X, Y = self.split_data(dataset)\n        X_subword, y_subword = self.add_subword2data(X, Y)\n        long_subword = [seq for seq in X_subword if len(seq) > self.MAX_LEN]\n        print(f\"Before cutting: \\nX_subword: {X_subword[0]}, \\nMax_seq: {max([len(line) for line in X_subword])}\\\n           \\nThe number of seq have len larger {self.MAX_LEN}: {len(long_subword)} \\nThe number of total seq: {len(X_subword)}\")\n        X_subword_at, y_subword_at = [], []\n        for i in range(len(X_subword)):\n            res_x, res_y = self.cutting_subword(X_subword[i], y_subword[i])\n            X_subword_at += res_x\n            y_subword_at += res_y\n        long_subword_at = [seq for seq in X_subword_at if len(seq) > self.MAX_LEN]\n        print(f\"After cutting: \\nX_subword: {X_subword_at[0]}, \\nMax_seq: {max([len(line) for line in X_subword_at])}\\\n           \\nThe number of seq have len larger: {self.MAX_LEN}: {len(long_subword_at)} \\nThe number of total seq: {len(X_subword_at)}\")\n        X_padding, y_padding, attention_masks = self.padding_data(X_subword_at, y_subword_at)\n        X_tensor,y_tensor,masks = self.covert2tensor(X_padding, y_padding, attention_masks)\n        train_data = TensorDataset(X_tensor, masks, y_tensor)\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = self.BATCH_SIZE)\n        return train_dataloader, labels","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:32.128112Z","iopub.execute_input":"2021-12-15T05:30:32.128834Z","iopub.status.idle":"2021-12-15T05:30:32.163258Z","shell.execute_reply.started":"2021-12-15T05:30:32.128794Z","shell.execute_reply":"2021-12-15T05:30:32.162550Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dir_train = '../input/vlsp2021-newfinal/train_full_15t12_9h45.pkl'\ndir_dev = '../input/vlsp2021-newfinal/dev_full_15t12_9h45.pkl'\ndir_test = '../input/vlsp2021-newfinal/test_full_15t12_9h45.pkl'\ndir_demo = '../input/vlsp2021-newfinal/demo.pkl'\npre_train = {\n    'mbert': 'bert-base-multilingual-cased',\n    'xlmr_base': 'xlm-roberta-base',\n    'xlmr_large': 'xlm-roberta-large',\n    'phobert': 'vinai/phobert-base'\n}\nmodel_name = 'xlmr_softmax'\npre_trained_name = pre_train['xlmr_base']","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:32.164157Z","iopub.execute_input":"2021-12-15T05:30:32.165145Z","iopub.status.idle":"2021-12-15T05:30:32.176595Z","shell.execute_reply.started":"2021-12-15T05:30:32.165117Z","shell.execute_reply":"2021-12-15T05:30:32.175728Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(pre_trained_name, do_lower_case=False,use_fast=False)\nconfig = AutoConfig.from_pretrained(pre_trained_name, output_hidden_states=True)\nconfig.hidden_dropout_prob = bert_hidd_dropout\nconfig.attention_probs_dropout_prob = bert_att_dropout\nmodel = AutoModel.from_pretrained(pre_trained_name, config=config, add_pooling_layer=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:34.079507Z","iopub.execute_input":"2021-12-15T05:30:34.079776Z","iopub.status.idle":"2021-12-15T05:30:40.092814Z","shell.execute_reply.started":"2021-12-15T05:30:34.079747Z","shell.execute_reply":"2021-12-15T05:30:40.092042Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tag_values = ['PAD', 'ADDRESS','SKILL','EMAIL','PERSON','PHONENUMBER','MISCELLANEOUS','QUANTITY','PERSONTYPE',\n              'ORGANIZATION','PRODUCT','IP','LOCATION','O','DATETIME','EVENT', 'URL']\n# #Merge ADDRESS\n# tag_values = ['PAD','SKILL','EMAIL','PERSON','PHONENUMBER','MISCELLANEOUS','QUANTITY','PERSONTYPE',\n#               'ORGANIZATION','PRODUCT','IP','LOCATION','O','DATETIME','EVENT', 'URL']\ntag2idx = {t: i for i, t in enumerate(tag_values)}\ntag2idx","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:40.094767Z","iopub.execute_input":"2021-12-15T05:30:40.095034Z","iopub.status.idle":"2021-12-15T05:30:40.104245Z","shell.execute_reply.started":"2021-12-15T05:30:40.094997Z","shell.execute_reply":"2021-12-15T05:30:40.103384Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"TRAIN = BertDataLoader(dir_train, tokenizer, tag_values, device, is_train = True, bs = BS, maxlen = MAX_LEN)\nDEV = BertDataLoader(dir_dev, tokenizer, tag_values, device, is_train = False, bs = BS, maxlen = MAX_LEN)\nTEST = BertDataLoader(dir_test, tokenizer, tag_values, device, is_train = False, bs = BS, maxlen = MAX_LEN)\n#DEMO = BertDataLoader(dir_demo, tokenizer, tag_values, device, is_train = False, bs = BS, maxlen = MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:40.105686Z","iopub.execute_input":"2021-12-15T05:30:40.105953Z","iopub.status.idle":"2021-12-15T05:30:40.117080Z","shell.execute_reply.started":"2021-12-15T05:30:40.105916Z","shell.execute_reply":"2021-12-15T05:30:40.116274Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_loader, train_labels = TRAIN.create_dataloader()\ndev_loader, dev_labels = DEV.create_dataloader()\ntest_loader, test_labels = TEST.create_dataloader()\n#demo_loader, demo_labels = DEMO.create_dataloader()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:30:40.119260Z","iopub.execute_input":"2021-12-15T05:30:40.119599Z","iopub.status.idle":"2021-12-15T05:31:27.462730Z","shell.execute_reply.started":"2021-12-15T05:30:40.119565Z","shell.execute_reply":"2021-12-15T05:31:27.461894Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 2.Modelling","metadata":{}},{"cell_type":"code","source":"# eps = ['SKILL', 'PRODUCT', 'PERSONTYPE', 'MISC', 'EVENT', 'ADDRESS']\n# class_weights =  [1.2 if tag in eps else 1 for tag in tag_values]\n# # converting list of class weights to a tensor\n# weights= torch.tensor(class_weights,dtype=torch.float)\n# weights = weights.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseBertSoftmax(nn.Module):\n    def __init__(self, model, drop_out , num_labels , concat=True):\n        super(BaseBertSoftmax, self).__init__()\n        self.concat = concat\n        self.num_labels = num_labels\n        self.model = model\n        self.dropout = nn.Dropout(drop_out)\n#         if self.concat:\n#             self.lstm = nn.LSTM(4*768, 256, batch_first=True, dropout=0.2, bidirectional=True)\n#             self.classifier = nn.Linear(256*2, num_labels) # 4 last of layer\n#         else:\n#             self.lstm = nn.LSTM(768, 256, batch_first=True, dropout=0.2, bidirectional=True)\n#             self.classifier = nn.Linear(256*2, num_labels)\n\n        if self.concat:\n            self.classifier = nn.Linear(4*768, num_labels) # 4 last of layer\n        else:\n            self.classifier = nn.Linear(768, num_labels)\n        \n        \n    def forward_custom(self, input_ids, attention_mask=None,\n                        labels=None, head_mask=None):\n        outputs = self.model(input_ids = input_ids, attention_mask=attention_mask)\n        if self.concat:\n            sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)\n            sequence_output = self.dropout(sequence_output)\n        else:\n            sequence_output = self.dropout(outputs[0])\n            \n        \n#         if self.concat:\n#             sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)\n#             lstm_output, (h,c) = self.lstm(sequence_output) ## extract the 1st token's embeddings\n#             hidden = torch.cat((lstm_output[:,-1, :256],lstm_output[:,0, 256:]),dim=-1)\n#             sequence_output = self.dropout(sequence_output)\n#         else:\n#             lstm_output, (h,c) = self.lstm(sequence_output[0]) ## extract the 1st token's embeddings\n#             hidden = torch.cat((lstm_output[:,-1, :256],lstm_output[:,0, 256:]),dim=-1)\n#             sequence_output = self.dropout(outputs[0])\n        #logits = self.linear(hidden.view(-1,256*2)) \n        \n\n        logits = self.classifier(sequence_output) # bsz, seq_len, num_labels\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n        return outputs  #scores, (hidden_states), (attentions)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:27.465124Z","iopub.execute_input":"2021-12-15T05:31:27.465326Z","iopub.status.idle":"2021-12-15T05:31:27.477574Z","shell.execute_reply.started":"2021-12-15T05:31:27.465300Z","shell.execute_reply":"2021-12-15T05:31:27.476872Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = BaseBertSoftmax(model=model, drop_out=linear_dropout, num_labels=len(tag_values))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:27.478879Z","iopub.execute_input":"2021-12-15T05:31:27.479399Z","iopub.status.idle":"2021-12-15T05:31:27.796628Z","shell.execute_reply.started":"2021-12-15T05:31:27.479343Z","shell.execute_reply":"2021-12-15T05:31:27.794888Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Exploring the parameters","metadata":{}},{"cell_type":"code","source":"# #!pip install prettytable\n# from prettytable import PrettyTable\n\n# def count_parameters(model):\n#     table = PrettyTable([\"Modules\", \"Parameters\"])\n#     total_params = 0\n#     for name, parameter in model.named_parameters():\n#         if not parameter.requires_grad: continue\n#         param = parameter.numel()\n#         table.add_row([name, param])\n#         total_params+=param\n#     print(table)\n#     print(f\"Total Trainable Params: {total_params}\")\n#     return total_params\n    \n# count_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:27.798746Z","iopub.execute_input":"2021-12-15T05:31:27.799016Z","iopub.status.idle":"2021-12-15T05:31:27.802468Z","shell.execute_reply.started":"2021-12-15T05:31:27.798979Z","shell.execute_reply":"2021-12-15T05:31:27.801787Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"cnt = -1\nnum_layer = 197\n\nfor param in model.named_parameters():\n    cnt += 1\n    if cnt>=num_layer:\n        param[1].requires_grad = True\n    else:\n        param[1].requires_grad = True\n    print(cnt,param[0],'\\t',param[1].requires_grad)\n\n\nFINETUNING = True\nif FINETUNING:\n    param_optimizer1 = list(model.named_parameters())[:num_layer]\n    param_optimizer2 = list(model.named_parameters())[num_layer:]\n    no_decay = ['bias', 'LayerNorm.weight'] #['bias', 'gamma', 'beta']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer1 if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': bert_weight_decay},\n        {'params': [p for n, p in param_optimizer1 if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0},\n        \n        {'params': [p for n, p in param_optimizer2 if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': softmax_weight_decay,\n         'lr': softmax_lr},\n        {'params': [p for n, p in param_optimizer2 if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0,\n         'lr':softmax_lr},\n    ]\n    \noptimizer = AdamW(\n    optimizer_grouped_parameters,\n    lr=bert_lr,\n    eps=1e-8\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:27.803788Z","iopub.execute_input":"2021-12-15T05:31:27.804240Z","iopub.status.idle":"2021-12-15T05:31:27.926497Z","shell.execute_reply.started":"2021-12-15T05:31:27.804203Z","shell.execute_reply":"2021-12-15T05:31:27.925829Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Evalutate","metadata":{}},{"cell_type":"code","source":"def show_span_f1(dic):\n    index = []\n    da = []\n    for tag, detail in dic.items():\n        index.append(tag)\n        da.append(detail)\n    df = pd.DataFrame(da)\n    df = df.set_index([pd.Index(index)])\n    return df\n\ndef convert_spanformat(arr):\n    if len(arr) < 1:\n        return None\n    text = ' '.join([i for i, j in arr])\n    pos = 0\n    start_end_labels = []\n    for word, tag in arr:\n        if len(start_end_labels) > 0 and tag == start_end_labels[-1][2]:\n            temp = [start_end_labels[-1][0], pos+len(word), tag]\n            start_end_labels[-1] = temp.copy()\n        else:\n            temp = [pos, pos+len(word), tag]\n            start_end_labels.append(temp)\n        pos += len(word) + 1\n\n    res = dict()   \n    for s, e, l in start_end_labels:\n        if l != 'O':\n            if l not in res:\n                res[l] = [(s, e)]\n            else:\n                res[l].append((s, e))\n    return res\n \ndef compare_span(span1, span2, res, strict= True):\n    all_labels = set(list(span1.keys()) + list(span2.keys()))\n    for l in all_labels:\n        if l not in res:\n            res[l] = [0, 0, 0, 0]\n        if l not in span1:\n            res[l][3] += len(span2[l])\n            continue\n        if l not in span2:\n            res[l][2] += len(span1[l])\n            continue\n        res[l][2] += len(span1[l])\n        res[l][3] += len(span2[l])\n        for s, e in span1[l]:\n            for s1, e1 in span2[l]:\n                temp0, temp1 = iou_single(s, e, s1, e1)\n                if strict:\n                    temp0, temp1 = int(temp0), int(temp1)\n                res[l][0] += temp0\n                res[l][1] += temp1\n    return res\n \ndef iou_single(s1, e1, s2, e2):\n    smax = max(s1, s2)\n    emin = min(e1, e2)\n    return max(0, emin - smax) / (e1 - s1) if e1 - s1 > 0 else 0, max(0, emin - smax) / (e2 - s2) if e2 - s2 > 0 else 0\n \n# (token - True - pred) \n# [[ ],[ ]]           \ndef span_f1(arr, labels = None, strict=True, digit=4):\n    all_labels = set()\n    dictt = dict()\n    for ar in arr:\n        text, gt, pred = list(zip(*ar))\n        gtSpan = convert_spanformat(list(zip(text, gt)))\n        predSpan = convert_spanformat(list(zip(text, pred)))\n        dictt = compare_span(predSpan, gtSpan, dictt, strict)\n\n        all_labels.update(list(gtSpan.keys()))\n    classfication_rp = dict()\n    # print(dictt)\n    f1_avg = 0\n    if labels is None:\n        labels = all_labels\n    for i in labels:\n        precision = dictt[i][0] / dictt[i][2] if dictt[i][2] > 0 else 0\n        recall = dictt[i][1] / dictt[i][3] if dictt[i][3] > 0 else 0\n        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n        classfication_rp[i] = {'precision': round(precision, digit), 'recall': round(recall, digit), 'f1': round(f1, digit), 'support': dictt[i][3]}\n        f1_avg += f1\n    return f1_avg / len(labels), classfication_rp\n\ndef merge_subtags_3column(tokens, tags_true, tags_predict, model_name):\n    tags = []\n    tests = []\n    trues = []\n    if 'mbert' in model_name:\n        for index in range(len(tokens)):\n            if \"##\" not in tokens[index]:\n                tags.append(tags_predict[index])\n                tests.append(tokens[index])\n                trues.append(tags_true[index])\n            else:\n                tests[-1] = tests[-1] + tokens[index].replace(\"##\",\"\")\n    elif 'phobert' in model_name:\n        for index in range(len(tokens)):\n            if len(tests) == 0:\n                tests.append(tokens[index])\n                tags.append(tags_predict[index])\n                trues.append(tags_true[index])\n            elif \"@@\" in tests[-1]:\n                tests[-1] = tests[-1][:-2] + tokens[index]\n            else:\n                tests.append(tokens[index])\n                tags.append(tags_predict[index])\n                trues.append(tags_true[index])\n    elif 'xlmr' in model_name:\n        for index in range(len(tokens)):\n            if len(tests) == 0:\n                if \"▁\" in tokens[index]:\n                    tests.append(tokens[index][1:])\n                else:\n                    tests.append(tokens[index])\n                tags.append(tags_predict[index])\n                trues.append(tags_true[index])\n            elif \"▁\" in tokens[index]:\n                tests.append(tokens[index][1:])\n                tags.append(tags_predict[index])\n                trues.append(tags_true[index])\n            else:\n                tests[-1] = tests[-1] + tokens[index]\n    return tests, trues, tags","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:27.927899Z","iopub.execute_input":"2021-12-15T05:31:27.928143Z","iopub.status.idle":"2021-12-15T05:31:27.959108Z","shell.execute_reply.started":"2021-12-15T05:31:27.928110Z","shell.execute_reply":"2021-12-15T05:31:27.958429Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, is_train = False):\n        model.eval()\n        eval_loss = 0\n        predictions_f1 , true_labels_f1 = [], []\n        out = []\n        for batch in dataloader:\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask, b_labels = batch\n            with torch.no_grad():\n                outputs = model.forward_custom(b_input_ids, b_input_mask, b_labels)\n            eval_loss += outputs[0].mean().item()\n            \n            label_ids = b_labels.to('cpu').numpy().tolist()\n            b_input_ids = b_input_ids.to('cpu').numpy().tolist()\n            if 'crf' in model_name:\n                predict_labels = outputs[1]\n            else:\n                predict_labels = []\n                logits = outputs[1].detach().cpu().numpy()\n                for predicts in np.argmax(logits, axis=2):\n                    predict_labels.append(predicts)\n\n            for b_input_id, preds, labels in zip(b_input_ids, predict_labels, label_ids):\n                n = sum(np.array(b_input_id) != 0)\n                tokens = tokenizer.convert_ids_to_tokens(b_input_id)[:n]\n                labels = [tag_values[i] for i in labels][:n]\n                preds = [tag_values[i] for i in preds]\n                token_new, label_new, pred_new = merge_subtags_3column(tokens, labels, preds, model_name)\n                #(token - label - pred)\n                temp = list(zip(token_new, label_new, pred_new))\n                out.append(temp)   \n                for _ , pred, label in zip(token_new, pred_new, label_new):\n                    predictions_f1.extend([pred])\n                    true_labels_f1.extend([label])\n        eval_loss = eval_loss / len(dataloader)\n        print(f\"Validation loss: {eval_loss:.4f}\")\n        LABEL = list(set(true_labels_f1))\n        print(\"Classification_report:\\n {}\".format(classification_report(true_labels_f1, predictions_f1, labels =  LABEL, digits=4)))\n#         if is_train:\n#             f1 = f1_score(true_labels_f1, predictions_f1, labels =  LABEL,average='macro')\n#             utils.summary_result(y_true = true_labels_f1 , y_pred = predictions_f1, is_show= is_show_cm)\n#             return f1,eval_loss\n#         #utils.summary_result(y_true = true_labels_f1 , y_pred = predictions_f1, is_show= is_show_cm)\n        return span_f1(out, labels = None, strict=True, digit=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:27.960503Z","iopub.execute_input":"2021-12-15T05:31:27.960752Z","iopub.status.idle":"2021-12-15T05:31:27.975704Z","shell.execute_reply.started":"2021-12-15T05:31:27.960716Z","shell.execute_reply":"2021-12-15T05:31:27.974426Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, dev_loader, scheduler, epochs=60, patience=15, max_grad_norm=1.0, PATH='ner_model.pt', dev_labels=None):\n    ## Store the average loss after each epoch so we can plot them.\n    train_loss_values, valid_loss_values = [], []\n    f1_max = 0\n    loss_min = np.Inf\n    f1_train_list, f1_dev_list = [], []\n    history = {}\n    #loss_path =  PATH[:-3] + '_loss.pt'\n    #f1_path = PATH[:-3] + '_f1.pt'\n    for epoch in trange(epochs, desc=\"Epoch\"):\n        # ========================================\n        #               Training\n        # ========================================\n        model.train()\n        # Training loop\n        train_loss = 0\n        train_pred , train_true = [], []\n        for step, batch in enumerate(train_loader):\n            # add batch to gpu\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask, b_labels = batch\n            # Always clear any previously calculated gradients before performing a backward pass.\n            model.zero_grad()\n\n            outputs = model.forward_custom(input_ids=b_input_ids, attention_mask=b_input_mask, \n                                           labels=b_labels, head_mask=None)\n            #################################\n            logits = outputs[1].detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            # Calculate the accuracy for this batch of test sentences.\n            train_pred.extend([list(p) for p in np.argmax(logits, axis=2)])\n            train_true.extend(label_ids)\n            ################################\n            loss = outputs[0]\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n            # track train loss\n            train_loss += loss.item()\n            # Clip the norm of the gradient\n            # This is to help prevent the \"exploding gradients\" problem.\n            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n            # update parameters\n            optimizer.step()\n            # Update the learning rate.\n            scheduler.step()\n        avg_train_loss = train_loss / len(train_loader)\n        train_loss_values.append(avg_train_loss)\n        ################################\n        train_pred_tags = [tag_values[p_i] for p, l in zip(train_pred, train_true)\n                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n        train_true_tags = [tag_values[l_i] for l in train_true\n                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n        f1_train = f1_score(y_true=train_true_tags, y_pred=train_pred_tags, labels=dev_labels, average='macro')\n        # ========================================\n        #               Validation\n        # ========================================\n        f1_dev, result = evaluate(model, dev_loader, is_train = False)\n        show_span_f1(result)\n        \n        f1_train_list.append(f1_train)\n        f1_dev_list.append(f1_dev)\n\n        history['train_loss_values'] = train_loss_values\n        history['valid_loss_values'] = valid_loss_values\n        history['f1_train_list'] = f1_train_list\n        history['f1_dev_list'] = f1_dev_list\n\n        if f1_dev > f1_max:\n            print(f'f1_span improved from: {f1_max:.4f} to {f1_dev:.4f}')\n            print(f'Best model saved to {PATH}')\n            f1_max = f1_dev\n            torch.save(model.state_dict(), PATH)\n            epochs_no_improve = 0\n            best_epoch = epoch\n        else:\n            print(f'f1_score dont improve from: {f1_max:.4f} to {f1_dev:.4f}')\n            epochs_no_improve += 1\n            if epochs_no_improve < patience:\n                print(f'EarlyStopping count: {epochs_no_improve}/{patience}')\n            else:\n                print(f'\\nEarly Stopping! Total epochs: {epochs}. Best epoch: {best_epoch} with f1_score: {f1_max:.4f}')\n                break\n    model.load_state_dict(torch.load(PATH), strict=False)\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:27.977378Z","iopub.execute_input":"2021-12-15T05:31:27.977888Z","iopub.status.idle":"2021-12-15T05:31:27.995745Z","shell.execute_reply.started":"2021-12-15T05:31:27.977851Z","shell.execute_reply":"2021-12-15T05:31:27.994905Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"PATH = './xlmr_full_025_025.pt'\nepochs = 50\npatience = 10\nmax_grad_norm = 1.0\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=int(total_steps/10), num_training_steps=total_steps)\nmodel, history = train_model(model, train_loader, dev_loader, scheduler, epochs, patience, max_grad_norm, PATH, dev_labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T05:31:53.653225Z","iopub.execute_input":"2021-12-15T05:31:53.653947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checkout the history\n#pd.DataFrame(history).plot(figsize=(10,7), xlabel=\"epochs\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history['train_loss_values'][:-10], label='train_loss')\nplt.plot(history['valid_loss_values'][:-10], label='dev_loss')\nplt.legend()\nplt.show()\nplt.plot(history['f1_train_list'][:-10], label='f1_train')\nplt.plot(history['f1_dev_list'][:-10], label='f1_dev')\nplt.xlabel('epochs')\nplt.ylabel('values')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#f1_path = './xlmr_softmax_f1.pt'\n#loss_path = './xlmr_softmax_loss.pt'\ndef evaluation(model, weight_path, data_loader, labels):\n    #model.load_state_dict(torch.load(weight_path), strict=False)\n    model.eval()\n    # Reset the validation loss for this epoch.\n    eval_loss, eval_accuracy = 0, 0\n    dev_pred , dev_true = [], []\n    for batch in data_loader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Telling the model not to compute or store gradients,\n        # saving memory and speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have not provided labels.\n            outputs =  model.forward_custom(input_ids=b_input_ids, attention_mask=b_input_mask, \n                                       head_mask=None, labels=b_labels)\n        # Move logits and labels to CPU\n\n        logits = outputs[1].detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences.\n        eval_loss += outputs[0].mean().item()\n        dev_pred.extend([list(p) for p in np.argmax(logits, axis=2)])\n        dev_true.extend(label_ids)\n\n    #############################################\n    dev_pred_tags = [tag_values[p_i] for p, l in zip(dev_pred, dev_true)\n                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n    dev_true_tags = [tag_values[l_i] for l in dev_true\n                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n    report = classification_report(y_true=dev_true_tags, y_pred=dev_pred_tags, labels = labels, digits = 4)\n    return report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## F1 classification_report","metadata":{}},{"cell_type":"code","source":"train_report = evaluation(model, PATH, train_loader, train_labels)\nprint(f'F1_report: \\n{train_report}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_report = evaluation(model, PATH, dev_loader, dev_labels)\nprint(f'F1_report: \\n{dev_report}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = evaluation(model, PATH, test_loader, test_labels)\nprint(f'F1_report: \\n{test_report}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download File\n<a href=\"./xlmr_full_025_025.pt\"> Download File </a>","metadata":{}},{"cell_type":"markdown","source":"#### Evaluation","metadata":{}}]}