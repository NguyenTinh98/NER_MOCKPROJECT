from pyvi import ViTokenizer, ViPosTagger
import re



from sklearn.metrics import *
import string
import unicodedata
import re
import numpy as np
import pickle

def process_unk(tokenizer, sq):
    temp = []
    for i in sq.split():
        if ['[UNK]'] == tokenizer.tokenize(i):
            temp.append(i[0]+i[1:].lower())
        else:
            temp.append(i)
    return ' '.join(temp)


def replace_special_character(text):
  sent_out = ""
  dictt = {'‚Ñ¢': ' ', '‚Äò': "'", '¬Æ': ' ', '√ó': ' ', 'üòÄ': ' ', '‚Äë': ' - ','ÃÅ': ' ', '‚Äî': ' - ', 'Ã£': ' ', '‚Äì': ' - ', '`': "'",\
             '‚Äú': '"', 'Ãâ': ' ','‚Äô': "'", 'ÃÉ': ' ', '\u200b': ' ', 'ÃÄ': ' ', '‚Äù': '"', '‚Ä¶': '...', '\ufeff': ' ', '‚Ä≥': '"'}
    
  for i in text:
    if i.isalnum() or i in string.punctuation or i == ' ':

        sent_out += i
    elif i in dictt:
        sent_out += dictt[i]
  return sent_out

def preprocessing_text(text):
    sent_out = ""
    stack = []
  
    sents = text.split('\n')
    for index in range(len(sents)):
      sent = sents[index]
      part_sent = ""
      
      if sent.strip() != '':
        for word in sent.split(' '):
          if word.strip() != '':
            word = unicodedata.normalize('NFKC', word)
            word = handle_character(word)
            word = handle_bracket(word)
            word = replace_special_character(word)
            part_sent +=  word + ' '
        
        
        
        sent_out += part_sent + " . "
        sent_out = re.sub(' +', ' ', sent_out)
        stack.append(len(sent_out.split(' '))  - 2)
    return {"sent_out":sent_out.rstrip(), "stack": stack }
def handle_bracket(test_str):
    res = re.findall(r'(\(|\[|\"|\'|\{)(.*?)(\}|\'|\"|\]|\))', test_str)
    if len(res) > 0:
        for r in res:
            sub_tring = "".join(r)
            start_index = test_str.find(sub_tring)
            end_index = start_index + len(r[1])
            test_str = test_str[: start_index+ 1] + " " + test_str[start_index+ 1:]
            test_str = test_str[: end_index + 2] + " " + test_str[end_index + 2:]
    return test_str

def handle_character(word):
    char_end = [".", ",", ";", "?", "+", ":" ]
    if word[-1] in char_end:
      word =  word[0:-1] + " " + word[-1]
    return word


def isNotSubword(x, idx, sub = '##'):
    return sub in x[idx] and idx < len(x) - 1 and sub in x[idx+1]

def cutting_subword(X, sub = '##', size=256):
    res_X = []
    punct = '.!?'
    st = 0
    cur = 0
    while (st < len(X)-size):
        flag = True
        for i in range(st+size-1, st-1, -1):
            if X[i] in punct and isNotSubword(X, i, sub):
                cur = i+1
                flag = False
                break
        if flag:
            for i in range(st+size-1, st-1, -1):
                if isNotSubword(X, i, sub):
                    cur = i+1
                    break
        if st == cur:
            cur += size
        res_X.append(X[st: cur])
        st = cur
    res_X.append(X[cur:])
    return res_X



def merge_word(sent, pred_out):
  '''
    :sent: is input sentences (hanlded pre-processing). example: 'pham van manh have email ( pvm26042000@gmail.com ) ....'
    :out : is input of predict, is list tuple. example: [('pham', 'O'), ('van', 'O'), ('manh', 'O'), ('have', 'O'), ('email', 'O'), ('(', 'O'),  ('pvm26042000', 'EMAIL'), ('@', 'EMAIL'),('gmail', 'EMAIL'), ('.', 'EMAIL'),('com', 'EMAIL'),(')', 'O'),('....', 'O')]
  '''
  out_merged = []
  parts = sent.split()

  for index in range(0, len(parts)):
    word = parts[index]

    
    for jndex in range(1, len(pred_out) + 1):
      token = pred_out[0:jndex]
      ws_token, ls_token = list(zip(*token))
      word_token = "".join(ws_token)
      if word_token == word:
        if len(token) == 1:
          out_merged.append(token[0])
        elif len(token) > 1:
          a, b = list(zip(*token))
          word_merged = "".join(a)
          l_merged = decide_label((word_merged, b))
          out_merged.append(l_merged)
        pred_out = pred_out[jndex:]
        break
  return out_merged


### check url
from tld import get_tld
def get_origin_domain(token):
  try:
    original = get_tld(token, as_object=False)
    return original.fld
  except:
    # print("can't get original domain")
    return None
    
def is_URL(token, black_list = [".exe",".txt", ".jpg", ".png", ".mp3 "], head_url = ["http://","https://", "www.", "localhost://"]):
  try:
    token = token.lower()
    orig_domain = get_origin_domain(token)
    if orig_domain != None:
      for tk in black_list:
        if tk in orig_domain:
          return False
    
    for tk in head_url:
      if tk in token:
        return True
      
    domains = re.findall(r'\b((?:https?://)?(?:(?:www\.)?(?:[\da-z\.-]+)\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\w\.-]*)*/?)\b', token)
    if len(domains) > 0 and  len(domains[0]) == len(token):
        return True
    return False
  except:
      print("error server!!!!")
      return False

  
def is_Email(token):
    emails = re.findall(r"[\w.+-]+@[\w-]+\.[\w.-]+", token)
    
    if len(emails) > 0 and len(emails[0]) == len(token):
        return True 
    return False

def is_IP(token):
  ipv4 = r"[0-9*a-z]{1,3}\.[0-9*a-z]{1,3}\.[0-9*a-z]{1,3}\.[0-9*a-z]{1,3}\:*[\d]*"
  ipv4Bi = r"[0-1*a-z]{8}\.[0-1*a-z]{8}\.[0-1*a-z]{8}\.[0-1*a-z]{8}\:*[\d]*"
  ipv6 = r'[A-Fa-f0-9:]+\:+[A-Fa-f0-9]+'

  ips = re.findall(ipv4 + '|' + ipv4Bi + '|' + ipv6, token)
  if len(ips) > 0 and len(ips[0]) == len(token):
        return True 
  return False

def processing_clear_label(datas):
  datas_trained = []
  for i in range(len(datas)):
    data = datas[i]

    if data[1] == 'EMAIL':
      if is_Email(data[0]) == False:
        data = (data[0], 'O')

    if data[1] == "URL":
      if is_URL(data[0]) == False:
        data = (data[0], 'O')

    if data[1] == 'IP':
    
      if is_IP(data[0]) == False:
        if data[0].isalnum():
          data = (data[0], 'QUANTITY')
        else:
          data = (data[0], 'O')

    if data[1] != "URL" and data[1] != "EMAIL" and data[1] != "IP":
  
      if is_Email(data[0]):
        data = (data[0], 'EMAIL')

      if is_URL(data[0]):
        data = (data[0], 'URL')

      if is_IP(data[0]):
        data = (data[0], 'IP')

    datas_trained.append(data)
  return datas_trained



def decide_label(part):
  word = part[0]
  labels = part[1]
  return (word, max(labels))


def cluster(data, maxgap):
  '''Arrange data into groups where successive elements
      differ by no more than *maxgap*
      >>> cluster([1, 6, 9, 100, 102, 105, 109, 134, 139], maxgap=10)
      [[1, 6, 9], [100, 102, 105, 109], [134, 139]]
      >>> cluster([1, 6, 9, 99, 100, 102, 105, 134, 139, 141], maxgap=10)
      [[1, 6, 9], [99, 100, 102, 105], [134, 139, 141]]
  '''
  black_list = [":", "(", ";", "{", "[", "v√†", "t·∫°i", "·ªü", "c·ªßa"]

  indexs = []
  for index in range(len(data)):
    token = data[index]
    if token[1] == "LOCATION" or token[1] == "ADDRESS" :
      indexs.append(index)

  if len(indexs) == 0:
    return list()

  indexs.sort()
  groups = [[indexs[0]]]

  for jndex in range(1,len(indexs)):
    x  = indexs[jndex]
    w, labels = list(zip(*data[indexs[jndex-1]:x]))
    if abs(x - groups[-1][-1]) <= maxgap and any(character in w for character in black_list) == False:
        groups[-1].append(x)
    elif any(character in data[indexs[jndex-1]:x] for character in black_list):
        groups.append([x])
    else:
        groups.append([x])
  return groups
  

def has_numbers(inputString):
  parts = inputString.split()

  for i in range(len(parts)):
    part = parts[i]
    for char in part:
      if char.isdigit():
        if i > 0 and parts[i-1].lower() in ['cn', "t·ªï", "qu·∫≠n", "q.", 'p.', 'ph∆∞·ªùng']:
          return False
        else:
          return True
  return False

def is_ADDRESS(string, label):
  index_not_dau_phay = [i for i, e in enumerate(label) if e == "O"]

  uy_tin = 0
  string_loc = " ".join(string)
  if 'ADDRESS' in label:
    uy_tin += 0.15

  if has_numbers(string_loc):
    uy_tin += 0.15

  for i in index_not_dau_phay:
      if string[i] not in [",", "-"]:
        uy_tin -= 0.05

      else:
        if string[i] == ",":
          uy_tin += 0.02

        if string[i] == "-":
          uy_tin += 0.05
    
  level = ["to√†_nh√†", "nh√†", "l·∫ßu", "t·∫ßng", "cƒÉn_h·ªô", "s·ªë", "l√¥", "km","qu·ªëc_l·ªô","ƒë·∫°i_l·ªô","kcn", "ƒë∆∞·ªùng","t·ªï", "ng√µ", "to√†", "ng√°ch", "h·∫ªm","ki·ªát", "chung_c∆∞", "·∫•p" ,"th√¥n", "khu","ph·ªë" , "qu·∫≠n", "ph∆∞·ªùng", "x√£", "th·ªã_x√£","huy·ªán", "th√†nh_ph·ªë", "tp", "t·ªânh" ]
  level_0 ={'status': True,'keywords': ["to√†", "to√†_nh√†", "nh√†", "l·∫ßu", "t·∫ßng", "cƒÉn_h·ªô", "chung_c∆∞", "s·ªë", "l√¥", "kcn", "km", "qu·ªëc_l·ªô", "ƒë·∫°i_l·ªô"] }
  level_1 = {'status': True, 'keywords': [ "ng√µ", "ng√°ch", "h·∫ªm","ki·ªát",]}
  level_2 = {'status': True, 'keywords':["·∫•p" ,"th√¥n", "khu","ph·ªë" , "qu·∫≠n", "ph∆∞·ªùng", "x√£", "t·ªï", "d√¢n_ph·ªë", "ƒë∆∞·ªùng"]}
  level_3 = {'status': True,'keywords':["th·ªã","huy·ªán"]}
  level_4 = {'status': True,'keywords':["th√†nh_ph·ªë", "tp", "t·ªânh"]}

  punct = ',;.-'

  parts =  ViPosTagger.postagging(ViTokenizer.tokenize(string_loc))[0]

  for i, seg_word in enumerate(parts):
    if seg_word.lower() in level:
      if seg_word.lower() in level_0['keywords'] and level_0['status'] == True and i < len(parts) - 1 and parts[i+1] not in punct:
        uy_tin += 0.25
        level_0['status'] = False

      elif seg_word.lower() in level_1['keywords'] and level_1['status'] == True and i < len(parts) - 1 and parts[i+1] not in punct:
        uy_tin += 0.075
        level_1['status'] = False

      elif seg_word.lower()  in level_2['keywords'] and level_2['status'] == True and i < len(parts) - 1 and parts[i+1] not in punct:
        uy_tin += 0.025
        level_2['status'] = False
      
      elif seg_word.lower() in  level_3['keywords'] and level_3['status'] == True and i < len(parts) - 1 and parts[i+1] not in punct:
        uy_tin += 0.015
        level_3['status'] = False

      
      elif seg_word.lower() in level_4['keywords'] and level_4['status'] == True and i < len(parts) - 1 and parts[i+1] not in punct:
        uy_tin += 0.01
        level_4['status'] = False

  return uy_tin >= 0.3

def post_processing(origin_sentence, stack, out_predict):

  out_merged = merge_word(origin_sentence, out_predict)
  datas_trained = processing_clear_label(out_merged)

  gr_indexs = cluster(datas_trained, 3)
  if len(gr_indexs) > 0:
    for index in gr_indexs:
      if len(index) > 1:
        string, label = list(zip(*datas_trained[index[0]: index[-1] + 1]))
        set_label = set(label)
        if len(set_label) >= 2 and (set_label != {'O', 'LOCATION'} and set_label != {'PERSONTYPE', 'LOCATION'}): 
          #print(string)
          if is_ADDRESS(string, label) == True:
            for i in range(index[0], index[-1] + 1):
              datas_trained[i] =(datas_trained[i][0], "ADDRESS")
          else:
            for i in range(index[0], index[-1] + 1):
              
              if datas_trained[i][0] in ',':
                datas_trained[i] = (datas_trained[i][0], "O")
              elif datas_trained[i] not in ['-/']:
                datas_trained[i] =(datas_trained[i][0], "LOCATION")
  print(datas_trained, stack)
  for stk in stack:
    datas_trained[stk] = ('/n', datas_trained[stk][1])
  return datas_trained
    
def span_cluster(dts, pred=1):
    sent = list(dts)
    sent[0] = (sent[0][0], sent[0][pred])
    sent[-1] = (sent[-1][0], sent[-1][pred])
    for i in range(1, len(sent)-1):
        if sent[i-1][pred] in ['SKILL'] and (sent[i-1][pred] == sent[i+1][pred] or i == len(sent)-2 or sent[i-1][pred] == sent[i+2][pred]) and sent[i][pred] != sent[i-1][pred] and sent[i][-1][2] >= 0.1:
            sent[i] = (sent[i][0], sent[i-1][pred])
        else:
            sent[i] = (sent[i][0], sent[i][pred])
    return sent

# def post_processing(origin_sentence, out_predict):

#   out_merged = merge_word(origin_sentence, out_predict)
#   datas_trained = processing_clear_label(out_merged)

#   gr_indexs = cluster(datas_trained, 3)
#   if len(gr_indexs) > 0:
#     for index in gr_indexs:
#       string, label = list(zip(*datas_trained[index[0]: index[-1] + 1]))
#       if is_ADDRESS(string, label) == True:
#         for i in range(index[0], index[-1] + 1):
#           datas_trained[i] =(datas_trained[i][0], "ADDRESS")
#       else:
#         for i in range(index[0], index[-1] + 1):
#           if datas_trained[i][0] in ',':
#             datas_trained[i] = (datas_trained[i][0], "O")
#           else:
#             datas_trained[i] =(datas_trained[i][0], "LOCATION")
#   return datas_trained
