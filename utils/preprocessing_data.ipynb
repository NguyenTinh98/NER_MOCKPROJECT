{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c970f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8acd84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for convert from .muc files to .conll files\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stringRegex = \"<ENAMEX TYPE[^>]*>([^<]*)</ENAMEX>\"\n",
    "nestStringRegex = \"<ENAMEX TYPE[^>]*>([^<]*<ENAMEX TYPE[^>]*>([^<]*<ENAMEX TYPE[^>]*>([^<]*<ENAMEX TYPE[^>]*>[^<]*</ENAMEX>[^<]*|[^<]*)*</ENAMEX>[^<]*|[^<]*)*</ENAMEX>[^<]*)*</ENAMEX>\"\n",
    "\n",
    "\n",
    "def split_tokenize(sentence):\n",
    "    nestData = []\n",
    "    regex = re.compile(stringRegex)\n",
    "    nestRegex = re.compile(nestStringRegex)\n",
    "\n",
    "    i = 0\n",
    "    while nestRegex.search(sentence, i) is not None:\n",
    "        regexMatcher = nestRegex.search(sentence, i)\n",
    "        if regexMatcher is not None:\n",
    "            nestData.append(sentence[i:regexMatcher.start()])\n",
    "            nestData.append(regexMatcher.group())\n",
    "            i = regexMatcher.end() + 1\n",
    "    if i != len(sentence):\n",
    "        nestData.append(sentence[i:])\n",
    "    # print(\"nestData:\", nestData)\n",
    "    data = []\n",
    "    for d in nestData:\n",
    "        i = 0\n",
    "        while regex.search(d, i) is not None:\n",
    "            # print(\"có\")\n",
    "            if nestRegex.search(d, i) is not None:\n",
    "                data.append(d)\n",
    "                i = len(d)\n",
    "                break\n",
    "            else:\n",
    "                regexMatcher = regex.search(d, i)\n",
    "                if regexMatcher is not None:\n",
    "                    data.append(d[i:regexMatcher.start()])\n",
    "                    data.append(regexMatcher.group())\n",
    "                    # print(c.group())\n",
    "                    i = regexMatcher.end() + 1\n",
    "        if i != len(d):\n",
    "            data.append(d[i:])\n",
    "\n",
    "    tokens = []\n",
    "    for d in data:\n",
    "        if \"<ENAMEX\" in d:\n",
    "            tokens.append(d)\n",
    "        else:\n",
    "            for w in word_tokenize(d):\n",
    "                tokens.append(w)\n",
    "        # print(d)\n",
    "    # for t in tokens:\n",
    "    #     print(t)\n",
    "    return tokens\n",
    "\n",
    "def merger_nest_entities(sentence):\n",
    "    nestData_sent = []\n",
    "    regex = re.compile(stringRegex)\n",
    "    regexMatcher = regex.search(sentence)\n",
    "    textMatch = regexMatcher.group()\n",
    "    g = \"<ENAMEX TYPE=\\\"([^<]*)\\\">\"\n",
    "    e = re.compile(g)\n",
    "    labeledEntity = e.search(textMatch)\n",
    "    # print(regexMatcher.groups()[0], \"\\t\", labeledEntity.groups()[0])\n",
    "\n",
    "    ws = word_tokenize(regexMatcher.groups()[0])\n",
    "    nestData_sent.append([ws[0], \"B-\" + labeledEntity.groups()[0]])\n",
    "    textMerger = ws[0] + \"_\" + \"B-\" + labeledEntity.groups()[0] + \" \"\n",
    "    for w in range(1, len(ws)):\n",
    "        nestData_sent.append([ws[w], \"I-\" + labeledEntity.groups()[0]])\n",
    "        textMerger += ws[w] + \"_\" + \"I-\" + labeledEntity.groups()[0] + \" \"\n",
    "\n",
    "    # print(textMatch)\n",
    "\n",
    "    return textMatch, textMerger[:-1], nestData_sent\n",
    "\n",
    "def make_text(data):\n",
    "    text = \"\"\n",
    "    for d in data:\n",
    "        for w in d:\n",
    "            line = w[0] + \"\\t_\" + \"\\t_\"\n",
    "            for e in w[1]:\n",
    "                line += \"\\t\" + e\n",
    "            text += line + \"\\n\"\n",
    "\n",
    "        text += \"\\n\"\n",
    "    return text\n",
    "\n",
    "def make_form(data):\n",
    "    new_data = []\n",
    "    num_maximum_entities = 0\n",
    "    for d in data:\n",
    "        # print(\"sentence = \",d)\n",
    "        tokens = split_tokenize(d)\n",
    "        # this token includes the word(raw text), ner(not nested) and nested ner.\n",
    "        # định dạng ConLL 2003 gồm:\n",
    "        # cột 0     cột 1       cột 2           cột 3       cột 4->>>\n",
    "        # word      POS(k có)   Phrase(k có)    NER_main    Ner_extension\n",
    "        data_sent = []\n",
    "        for token in tokens:\n",
    "            # print(\"token = \", token)\n",
    "            # xử lý thực thể\n",
    "            if \"<ENAMEX\" in token:\n",
    "                nestRegex = re.compile(nestStringRegex)\n",
    "                regex = re.compile(stringRegex)\n",
    "                # thực thể đơn\n",
    "                # print(nestRegex.search(token))\n",
    "                if nestRegex.search(token) is None:\n",
    "                    # print(\"-> Nhãn đơn: \")\n",
    "                    regexMatcher = regex.search(token)\n",
    "\n",
    "                    textMatch = regexMatcher.group()\n",
    "                    g = \"<ENAMEX TYPE=\\\"([^<]*)\\\">\"\n",
    "                    e = re.compile(g)\n",
    "                    labeledEntity = e.search(textMatch)\n",
    "\n",
    "                    # print(regexMatcher.groups()[0], \"\\t\", labeledEntity.groups()[0])\n",
    "                    ws = word_tokenize(regexMatcher.groups()[0])\n",
    "                    data_sent.append(ws[0] + \"_\" + \"B-\"+labeledEntity.groups()[0])\n",
    "                    for w in range(1, len(ws)):\n",
    "                        data_sent.append(ws[w] + \"_\" + \"I-\" + labeledEntity.groups()[0])\n",
    "\n",
    "                # thực thể lồng ghép\n",
    "                else:\n",
    "                    # tái sử dụng hàm split_tokenize(sentence)\n",
    "                    # print(\"---> Nhãn lồng: \")\n",
    "                    # print(nestRegex.search(token).groups()[0])\n",
    "                    if nestRegex.search(token).groups()[0] is None:\n",
    "                        print(token)\n",
    "                        # continue\n",
    "                        sys.exit(\"Error: NER is not exits\")\n",
    "\n",
    "                    while nestRegex.search(token) is not None and regex.search(token) is not None:\n",
    "                        textMatch, textMerger, nestData_sent = merger_nest_entities(token)\n",
    "                        # print(\"textMatch = \",textMatch)\n",
    "\n",
    "                        g = token.replace(textMatch, textMerger)\n",
    "                        # print(\"g = \", g)\n",
    "                        token = g\n",
    "\n",
    "                    textMatch, textMerger, nestData_sent = merger_nest_entities(token)\n",
    "                    for d_s in nestData_sent:\n",
    "                        data_sent.append(d_s[0]+\"_\"+d_s[1])\n",
    "                        # print(d_s[0]+\"_\"+d_s[1])\n",
    "\n",
    "            else:\n",
    "                data_sent.append(token)\n",
    "        pass\n",
    "\n",
    "        for i in range(len(data_sent)):\n",
    "            labeledEntities = data_sent[i].split(\"_\")\n",
    "\n",
    "            word = labeledEntities[0]\n",
    "            entities = labeledEntities[1:]\n",
    "            # print(word, entities)\n",
    "            if num_maximum_entities < len(entities):\n",
    "                num_maximum_entities = len(entities)\n",
    "            entities = entities[::-1]\n",
    "\n",
    "            data_sent[i] = [word, entities]\n",
    "        pass\n",
    "        new_data.append(data_sent)\n",
    "        # split_tokenize(d, nestStringRegex)\n",
    "        # print()\n",
    "\n",
    "    for i in range(len(new_data)):\n",
    "        for j in range(len(new_data[i])):\n",
    "            # print(new_data[i])\n",
    "            while len(new_data[i][j][1]) < num_maximum_entities:\n",
    "                new_data[i][j][1].append(\"O\")\n",
    "\n",
    "    return make_text(new_data)\n",
    "\n",
    "\n",
    "def get_sentence_origin(sentence):\n",
    "    # tách các thành phần có chứa \"<ENAMEX ..\" ra khỏi câu:\n",
    "    return re.sub(r'<ENAMEX TYPE=\"[A-Z|\\-]*\">|</ENAMEX>', '', sentence).strip(\" \").replace(\"  \", \" \")\n",
    "\n",
    "\n",
    "def read_input_file(filename):\n",
    "#     if os.path.isfile(filename):\n",
    "#         print(filename)\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = []\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines)):\n",
    "            line = lines[i].strip(\"\\n\")\n",
    "            # if line != \"\":\n",
    "            text.append(line)\n",
    "            # text += lines[i]\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def write_output_file(filename, text):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "def convert(path_in, path_out):\n",
    "    list_files = os.listdir(path_in)\n",
    "    if not os.path.exists(path_out):\n",
    "        os.mkdir(path_out)\n",
    "    for file in tqdm(list_files):\n",
    "        file_path = path_in + \"/\" + file\n",
    "        data = read_input_file(file_path)\n",
    "        output = make_form(data)\n",
    "        file_out = path_out + \"/\" + file[:-3] + \"conll\"\n",
    "        write_output_file(file_out, output)\n",
    "#     write_output_file(path_out, path_out)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f99bc2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check dir_out is exist or not\n",
    "## if not, create empty dir_out\n",
    "## input: dir_out (not ended by /)\n",
    "\n",
    "def checkDir(dir_out):\n",
    "    if not os.path.exists(dir_out):\n",
    "        os.mkdir(dir_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eee6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace </ENAMEX>punc => </ENAMEX> punc\n",
    "## input: path of origin dir, path of destination dir (not ended by /)\n",
    "## output: None\n",
    "\n",
    "def add_space_tag(dir_in, dir_out):\n",
    "    a = ',:.?!'\n",
    "    aa = ['</ENAMEX>' + i for i in a]\n",
    "    aaa = ['</ENAMEX> ' + i for i in  a]\n",
    "\n",
    "    checkDir(dir_out)\n",
    "        \n",
    "    for path_in in tqdm(os.listdir(dir_in)):\n",
    "        dt = []\n",
    "        with open(dir_in + '/' + path_in, 'r', encoding='utf-8') as f:\n",
    "            for i in f.readlines():\n",
    "                for j in range(len(aa)):\n",
    "                    i = i.replace(aa[j], aaa[j])\n",
    "                dt.append(i)\n",
    "        with open(dir_out + '/' + path_in, 'w', encoding='utf-8') as f:\n",
    "            for i in dt:\n",
    "                f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc7ea215",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train, dev, test dts to folder resp: dir_out/train, dir_out/dev, dir_out/test\n",
    "## input: dir_in, dir_out (not ended by /)\n",
    "\n",
    "import shutil\n",
    "def train_dev_test_split(dir_in, dir_out):\n",
    "    checkDir(dir_out)\n",
    "    for i in ['train', 'test', 'dev']:\n",
    "        checkDir(dir_out + '/' + i)\n",
    "    \n",
    "    for i in tqdm(os.listdir(dir_in)):\n",
    "        if 'dev' in i:\n",
    "            shutil.move( dir_in + '/' + i, dir_out + '/dev/' +i)\n",
    "        elif 'test' in i:\n",
    "            shutil.move(dir_in + '/' + i, dir_out + '/test/' + i)\n",
    "        else:\n",
    "            shutil.move(dir_in + '/' + i, dir_out + '/train/' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0a4965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge all files into only file with name _merge.txt\n",
    "## input: dir_path contain all files need to merge\n",
    "\n",
    "def merge_files(dir_path):\n",
    "    temp = []\n",
    "    for i in os.listdir(dir_path):\n",
    "        with open(dir_path +'/' + i, 'r', encoding='utf-8') as f:\n",
    "            temp += f.readlines()\n",
    "    with open(dir_path + '_merge.txt', 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a616bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize merge file to atleast 6 columns format\n",
    "## input: file_path origin merge, file_path_new\n",
    "\n",
    "def convert2normalize(file_path, file_path_new):\n",
    "    print('convert ' + file_path + 'to atleast 6 columns format')\n",
    "    temp = []\n",
    "    with open( file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) == 3:\n",
    "                line.append('O')\n",
    "            if len(line) >= 4 and len(line) < 6:\n",
    "                for i in range(6-len(line)):\n",
    "                    line.append('?')\n",
    "            if len(line) != 1 and len(line) <4:\n",
    "                line = ['']\n",
    "            temp.append('\\t'.join(line))\n",
    "    with open(file_path_new, 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c191014",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert from 6 columns to 2 columns format\n",
    "## input: file_path orgin, file_path_new\n",
    "\n",
    "def convert2columns2(file_path, file_path_new):\n",
    "    print('convert ' + file_path + 'to 2 columns format')\n",
    "    temp = []\n",
    "    with open( file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) > 3:\n",
    "                temp.append('\\t'.join([line[0], line[3]]))\n",
    "            else:\n",
    "                if len(line) > 1:\n",
    "                    print(line, file_path, i)\n",
    "                temp.append('')\n",
    "    with open(file_path_new, 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d91cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge subtags and BI- tags\n",
    "## input: name of tag origin: string\n",
    "## output: name of merge-tag: string\n",
    "\n",
    "def merge_subtag(text):\n",
    "    NER = ['DATETIME', 'PERSONTYPE', 'PERSON', 'ORGANIZATION', 'PRODUCT', 'EVENT', 'LOCATION', 'URL', 'PHONENUMBER', 'QUANTITY', 'IP', 'ADDRESS', 'SKILL', 'EMAIL', 'MISCELLANEOUS']\n",
    "    for i in NER:\n",
    "        if len(i) > 1 and i in text:\n",
    "            return i\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "999acd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert data 2 columns to merge tag format 2 columns\n",
    "## input: file name origin, file_name new\n",
    "\n",
    "def convert_merge_subtag(file_path, file_new):\n",
    "    print('convert ' + file_path + 'to merge subtag format')\n",
    "    temp = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) > 1:\n",
    "                line[1] = merge_subtag(line[1])\n",
    "            temp.append('\\t'.join(line))\n",
    "    with open(file_new, 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae866896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert from 2 columns format to pkl file\n",
    "## input: file_txt 2 columns format\n",
    "## output: create pickle file with same name (change extension)\n",
    "\n",
    "def convert2pkl(file_txt):\n",
    "    print('convert ' + file_txt + 'to pkl')\n",
    "    temp = []\n",
    "    res = []\n",
    "    with open(file_txt, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                line = line.strip().split('\\t')\n",
    "                temp.append((line[0], line[1]))\n",
    "            else:\n",
    "                if len(temp) > 0:\n",
    "                    res.append(temp)\n",
    "                temp = []\n",
    "    new_file = file_txt.replace('txt', 'pkl')\n",
    "    with open(new_file, 'wb') as f:\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc8fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge BI tag\n",
    "## input: tag origin: string\n",
    "## output: merge tag: string\n",
    "\n",
    "def merge_BItag(tag):\n",
    "    tag = tag.replace('B-', '')\n",
    "    return tag.replace('I-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa31780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset from pickle file to dataframe\n",
    "## input: file path pickle, is_merge: boolean (merge BI-tag or not)\n",
    "## output: dataframe\n",
    "\n",
    "def read_dts(file_path, is_merge=True):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        for a, b in i:\n",
    "            X.append(a)\n",
    "            y.append(b)\n",
    "    df = pd.DataFrame(X, columns=['word'])\n",
    "    df['tag'] = y\n",
    "    if is_merge:\n",
    "        df['tag'] = df['tag'].apply(lambda x:merge_BItag(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7584070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create vocab of dataset with {word: {tag: count of tag}}\n",
    "## input: dataframe\n",
    "## output: vocab: dict\n",
    "\n",
    "def create_vocab_dict(df):\n",
    "    vocab = {}\n",
    "    tag = df.tag.unique()\n",
    "    temp = {}\n",
    "    for i in tag:\n",
    "        temp[i] =  0\n",
    "    for i in range(len(df)):\n",
    "        word, tag = df['word'][i], df['tag'][i]\n",
    "        if word not in vocab:\n",
    "            vocab[word] = temp.copy()\n",
    "        vocab[word][tag] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaebc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "colors = ['Apricot', 'Brown', 'Olive', 'Teal', 'Pink', 'Black', 'Red', 'Orange', 'Yellow', 'Lime', 'Green', 'Cyan', 'Blue', 'Purple', \n",
    "         'Grey']\n",
    "NER = ['DATETIME', 'PERSONTYPE', 'PERSON', 'ORGANIZATION', 'PRODUCT', 'EVENT', 'LOCATION', 'URL', 'PHONENUMBER', 'QUANTITY', 'IP', 'ADDRESS', 'SKILL', 'EMAIL', 'MISCELLANEOUS']\n",
    "COLORS = dict()\n",
    "for i in range(len(NER)):\n",
    "    COLORS[NER[i]] = colors[i]\n",
    "OPTIONS = {'ents': NER, 'colors': COLORS}\n",
    "    \n",
    "## visualize result\n",
    "## input: predict format [(word, tag)]\n",
    "\n",
    "def visualize_spacy(arr):\n",
    "    if len(arr) < 1:\n",
    "        return None\n",
    "    text = ' '.join([i for i, j in arr])\n",
    "    pos = 0\n",
    "    start_end_labels = []\n",
    "    for word, tag in arr:\n",
    "        if len(start_end_labels) > 0 and tag == start_end_labels[-1][2]:\n",
    "            temp = [start_end_labels[-1][0], pos+len(word), tag]\n",
    "            start_end_labels[-1] = temp.copy()\n",
    "        else:\n",
    "            temp = [pos, pos+len(word), tag]\n",
    "            start_end_labels.append(temp)\n",
    "        pos += len(word) + 1\n",
    "        \n",
    "    ex = [{'text': text, 'ents': [{'start': x[0], 'end': x[1], 'label': x[2]} for x in start_end_labels if x[2]!= 0]}]\n",
    "    displacy.render(ex, manual=True, jupyter=True, style='ent', options = OPTIONS )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06c131c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:00<00:00, 3133.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:19<00:00, 78.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:00<00:00, 7762.74it/s]\n"
     ]
    }
   ],
   "source": [
    "## pipeline load dts from all-muc-folder\n",
    "LIST_FOLDER = ['train', 'dev', 'test']\n",
    "DIR_MUC = 'MyNER21/NER-Data-Muc' ## contain all origin muc files, linked to your dataset.\n",
    "DIR_MUC_FIXED = 'demo_fixed'  ## contain all muc files fixed space, any name.\n",
    "DIR_CONLL =  'demo_conll' ## contain all conll files, any name.\n",
    "DIR_DTS =   'demo_dts' ## contain folders train, dev, test which each folder contains files conll after splitting.\n",
    "print('Add space after TAG HTML END and punc:')\n",
    "add_space_tag(DIR_MUC, DIR_MUC_FIXED)  ## add space after </ENAMEX>punc\n",
    "print('Convert muc format to conll format:')\n",
    "convert(DIR_MUC_FIXED, DIR_CONLL)  ## convert muc format to conll format\n",
    "print('Train dev test split:')\n",
    "train_dev_test_split(DIR_CONLL, DIR_DTS) ## split train, dev, test\n",
    "for i in LIST_FOLDER:\n",
    "    merge_files(DIR_DTS + '/' + i)     ## merge folder train, dev, test into only files with name DIR_DTS/train_merge.txt, DIR_DTS/dev_merge.txt, DIR_DTS/test_merge.txt\n",
    "    convert2normalize(DIR_DTS+'/'+i+'_merge.txt', DIR_DTS+'/'+i+'_fixed.txt')\n",
    "    convert2columns2(DIR_DTS+'/'+i+'_fixed.txt', DIR_DTS+'/'+i+'_2columns.txt')\n",
    "    convert_merge_subtag(DIR_DTS+'/'+i+'_2columns.txt', DIR_DTS+'/'+i+'_merge_subtag.txt')\n",
    "    convert2pkl(DIR_DTS+'/'+i+'_merge_subtag.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
