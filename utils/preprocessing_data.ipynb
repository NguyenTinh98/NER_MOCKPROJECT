{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def is_URL(token):\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    for word in token.split(\" \"):\n",
    "        # print(word)\n",
    "        domain = re.findall(r'\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b', word)\n",
    "        \n",
    "        if len(domain) != 0:\n",
    "            index_start_domain = word.find(domain[0]) + index\n",
    "            if word.find(domain[0]) == 0:\n",
    "                index_end_domain = index_start_domain + len(word)\n",
    "            else:\n",
    "                index_end_domain = index_start_domain + len(domain[0])\n",
    "            indexs.append((index_start_domain, index_end_domain))\n",
    "        index += len(word) + 1\n",
    "    return indexs\n",
    "\n",
    "def is_Email(token):\n",
    "    index = 0\n",
    "    indexs = []\n",
    "    for word in token.split(\" \"):\n",
    "        # print(word)\n",
    "        emails = re.findall(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", word)\n",
    "        # print(emails)\n",
    "        if len(emails) != 0:\n",
    "            index_start_email = word.find(emails[0]) + index\n",
    "            \n",
    "            index_end_email = index_start_email + len(emails[0])\n",
    "            \n",
    "            indexs.append((index_start_email, index_end_email))\n",
    "        index += len(word) + 1\n",
    "    return indexs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datas = []\n",
    "for index in range(0, len(data_pkl)):\n",
    "    data = []\n",
    "    # try:\n",
    "    sent = data_pkl[index]\n",
    "    for word in sent:\n",
    "        w = word[0]\n",
    "        ner = word[1]\n",
    "        if ner != \"O\" and len(data) > 0:\n",
    "            if ner  == data[-1][1].split(\"-\")[-1]:\n",
    "                data.append((w, \"I-\" + ner ))\n",
    "                # print(row[\"ner\"])\n",
    "            if ner != data[-1][1].split(\"-\")[-1]:\n",
    "                data.append((w, \"B-\" + ner ))\n",
    "        \n",
    "        elif ner != \"O\" and len(data) == 0:\n",
    "                data.append((w, \"B-\" + ner ))\n",
    "        else:\n",
    "            data.append((w, ner ))\n",
    "    datas.append(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "ViTokenizer.tokenize(u\"số nhà 20, thôn Cẩm Thành, Xã Cẩm La, Huyện Yên Hưng, Tỉnh Quảng Ninh\")\n",
    "\n",
    "# ViPosTagger.postagging(ViTokenizer.tokenize(u\"Trường đại học Bách Khoa Hà Nội\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'số nhà 20 , thôn Cẩm_Thành , Xã Cẩm_La , Huyện Yên_Hưng , Tỉnh Quảng_Ninh'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def concate_token(datas, labels_merge):\n",
    "    data = []\n",
    "    for word in datas:\n",
    "        ner = word[1].split(\"-\")[-1]\n",
    "\n",
    "        if (word[1] in labels_merge) and len(data) > 0:\n",
    "\n",
    "            ner_end = data[-1][1]\n",
    "     \n",
    "            if ner == ner_end:\n",
    "                segment_str= data[-1][0]  +word[0]\n",
    "                data[-1] = ( segment_str.strip(), ner)\n",
    "            else: \n",
    "                data.append((word[0].strip(), ner))\n",
    "        else:\n",
    "            data.append((word[0].strip(), ner))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "data_mergeds = []\n",
    "\n",
    "data_read_pkl = [[('manh', \"URL\"), ('.', \"URL\"), ('com', \"URL\")]]\n",
    "for data in data_read_pkl:\n",
    "    data_mergeds.append(concate_token(data, [\"URL\"]))\n",
    "data_mergeds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('manh.com', 'URL')]]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def handle_word_blacklist(sub_string):\n",
    "  black_token = \"_\"\n",
    "  indexs = [pos for pos, char in enumerate(sub_string) if char == black_token]\n",
    "  for index in indexs:\n",
    "\n",
    "    sub_string = replacer(sub_string, \"-\", index)\n",
    "  return sub_string, indexs\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "def handle_bracket(test_str):\n",
    "  res = re.findall(r'(\\(|\\[|\\\"|\\'|\\{)(.*?)(\\}|\\'|\\\"|\\]|\\))', test_str)\n",
    "  # print(res)\n",
    "  if len(res) > 0:\n",
    "    for r in res:\n",
    "      sub_tring = \"\".join(r)\n",
    "      start_index = test_str.find(sub_tring)\n",
    "      end_index = start_index + len(r[1])\n",
    "      test_str = test_str[: start_index+ 1] + \" \" + test_str[start_index+ 1:]\n",
    "      test_str = test_str[: end_index + 2] + \" \" + test_str[end_index + 2:]\n",
    "      # test_str = \n",
    "  return test_str\n",
    "\n",
    "\n",
    "def handle_character(sub_string):\n",
    "  char_end = [\".\", \",\", \";\", \"?\", \"+\", \":\" ]\n",
    "  count = 1\n",
    "  for index in reversed(range(len(sub_string))):\n",
    "\n",
    "    \n",
    "    # print(index)\n",
    "    c = sub_string[index]\n",
    "    # print(index, c)\n",
    "\n",
    "    #check black token\n",
    "\n",
    "    if c not in char_end:\n",
    "      break\n",
    "    \n",
    "    elif c in char_end:\n",
    "      # print(sent[index -1] )\n",
    "      if sub_string[index -1] not in char_end:\n",
    "        # print(sub_string[index -1])\n",
    "        sub_string = sub_string[:index] + \" \" + sub_string[index:]\n",
    "        count = 2\n",
    "        break\n",
    "\n",
    "  return sub_string, count\n",
    "\n",
    "\n",
    "  def preprocess_data(sent):\n",
    "  sent = handle_bracket(sent)\n",
    "  sent = re.sub(' +', ' ', sent)\n",
    "  sent_out = \"\"\n",
    "  stack = {'ents' : {}}\n",
    "  index_count = 0\n",
    "  parts = sent.split()\n",
    "\n",
    "  for index in range(len(parts)):\n",
    "    word_space = parts[index]\n",
    "    # print(word_space)\n",
    "\n",
    "    sub_string_handeled, count = handle_character(word_space)\n",
    "    \n",
    "    sub_string_handeled, indexs = handle_word_blacklist(sub_string_handeled)\n",
    "    if len(indexs) > 0:\n",
    "      stack['ents'][index_count] = indexs\n",
    "    index_count += count\n",
    "      \n",
    "\n",
    "    if index != len(parts) - 1:\n",
    "      sent_out +=  sub_string_handeled + \" \"\n",
    "    else:\n",
    "      sent_out += sub_string_handeled\n",
    " \n",
    "  stack['sent'] = sent_out\n",
    "  return stack"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def is_Email(toekn):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text)\n",
    "    return emails"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = \"Please contact us at contact@tutorialspoint.com for further information.\"+\\\n",
    "        \" You can also give feedbacl at feedback@tp.com\"\n",
    "is_Email(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "## functions for convert from .muc files to .conll files\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stringRegex = \"<ENAMEX TYPE[^>]*>([^<]*)</ENAMEX>\"\n",
    "nestStringRegex = \"<ENAMEX TYPE[^>]*>([^<]*<ENAMEX TYPE[^>]*>([^<]*<ENAMEX TYPE[^>]*>([^<]*<ENAMEX TYPE[^>]*>[^<]*</ENAMEX>[^<]*|[^<]*)*</ENAMEX>[^<]*|[^<]*)*</ENAMEX>[^<]*)*</ENAMEX>\"\n",
    "\n",
    "\n",
    "def split_tokenize(sentence):\n",
    "    nestData = []\n",
    "    regex = re.compile(stringRegex)\n",
    "    nestRegex = re.compile(nestStringRegex)\n",
    "\n",
    "    i = 0\n",
    "    while nestRegex.search(sentence, i) is not None:\n",
    "        regexMatcher = nestRegex.search(sentence, i)\n",
    "        if regexMatcher is not None:\n",
    "            nestData.append(sentence[i:regexMatcher.start()])\n",
    "            nestData.append(regexMatcher.group())\n",
    "            i = regexMatcher.end() + 1\n",
    "    if i != len(sentence):\n",
    "        nestData.append(sentence[i:])\n",
    "    # print(\"nestData:\", nestData)\n",
    "    data = []\n",
    "    for d in nestData:\n",
    "        i = 0\n",
    "        while regex.search(d, i) is not None:\n",
    "            # print(\"có\")\n",
    "            if nestRegex.search(d, i) is not None:\n",
    "                data.append(d)\n",
    "                i = len(d)\n",
    "                break\n",
    "            else:\n",
    "                regexMatcher = regex.search(d, i)\n",
    "                if regexMatcher is not None:\n",
    "                    data.append(d[i:regexMatcher.start()])\n",
    "                    data.append(regexMatcher.group())\n",
    "                    # print(c.group())\n",
    "                    i = regexMatcher.end() + 1\n",
    "        if i != len(d):\n",
    "            data.append(d[i:])\n",
    "\n",
    "    tokens = []\n",
    "    for d in data:\n",
    "        if \"<ENAMEX\" in d:\n",
    "            tokens.append(d)\n",
    "        else:\n",
    "            for w in word_tokenize(d):\n",
    "                tokens.append(w)\n",
    "        # print(d)\n",
    "    # for t in tokens:\n",
    "    #     print(t)\n",
    "    return tokens\n",
    "\n",
    "def merger_nest_entities(sentence):\n",
    "    nestData_sent = []\n",
    "    regex = re.compile(stringRegex)\n",
    "    regexMatcher = regex.search(sentence)\n",
    "    textMatch = regexMatcher.group()\n",
    "    g = \"<ENAMEX TYPE=\\\"([^<]*)\\\">\"\n",
    "    e = re.compile(g)\n",
    "    labeledEntity = e.search(textMatch)\n",
    "    # print(regexMatcher.groups()[0], \"\\t\", labeledEntity.groups()[0])\n",
    "\n",
    "    ws = word_tokenize(regexMatcher.groups()[0])\n",
    "    nestData_sent.append([ws[0], \"B-\" + labeledEntity.groups()[0]])\n",
    "    textMerger = ws[0] + \"_\" + \"B-\" + labeledEntity.groups()[0] + \" \"\n",
    "    for w in range(1, len(ws)):\n",
    "        nestData_sent.append([ws[w], \"I-\" + labeledEntity.groups()[0]])\n",
    "        textMerger += ws[w] + \"_\" + \"I-\" + labeledEntity.groups()[0] + \" \"\n",
    "\n",
    "    # print(textMatch)\n",
    "\n",
    "    return textMatch, textMerger[:-1], nestData_sent\n",
    "\n",
    "def make_text(data):\n",
    "    text = \"\"\n",
    "    for d in data:\n",
    "        for w in d:\n",
    "            line = w[0] + \"\\t_\" + \"\\t_\"\n",
    "            for e in w[1]:\n",
    "                line += \"\\t\" + e\n",
    "            text += line + \"\\n\"\n",
    "\n",
    "        text += \"\\n\"\n",
    "    return text\n",
    "\n",
    "def make_form(data):\n",
    "    new_data = []\n",
    "    num_maximum_entities = 0\n",
    "    for d in data:\n",
    "        # print(\"sentence = \",d)\n",
    "        tokens = split_tokenize(d)\n",
    "        # this token includes the word(raw text), ner(not nested) and nested ner.\n",
    "        # định dạng ConLL 2003 gồm:\n",
    "        # cột 0     cột 1       cột 2           cột 3       cột 4->>>\n",
    "        # word      POS(k có)   Phrase(k có)    NER_main    Ner_extension\n",
    "        data_sent = []\n",
    "        for token in tokens:\n",
    "            # print(\"token = \", token)\n",
    "            # xử lý thực thể\n",
    "            if \"<ENAMEX\" in token:\n",
    "                nestRegex = re.compile(nestStringRegex)\n",
    "                regex = re.compile(stringRegex)\n",
    "                # thực thể đơn\n",
    "                # print(nestRegex.search(token))\n",
    "                if nestRegex.search(token) is None:\n",
    "                    # print(\"-> Nhãn đơn: \")\n",
    "                    regexMatcher = regex.search(token)\n",
    "\n",
    "                    textMatch = regexMatcher.group()\n",
    "                    g = \"<ENAMEX TYPE=\\\"([^<]*)\\\">\"\n",
    "                    e = re.compile(g)\n",
    "                    labeledEntity = e.search(textMatch)\n",
    "\n",
    "                    # print(regexMatcher.groups()[0], \"\\t\", labeledEntity.groups()[0])\n",
    "                    ws = word_tokenize(regexMatcher.groups()[0])\n",
    "                    data_sent.append(ws[0] + \"_\" + \"B-\"+labeledEntity.groups()[0])\n",
    "                    for w in range(1, len(ws)):\n",
    "                        data_sent.append(ws[w] + \"_\" + \"I-\" + labeledEntity.groups()[0])\n",
    "\n",
    "                # thực thể lồng ghép\n",
    "                else:\n",
    "                    # tái sử dụng hàm split_tokenize(sentence)\n",
    "                    # print(\"---> Nhãn lồng: \")\n",
    "                    # print(nestRegex.search(token).groups()[0])\n",
    "                    if nestRegex.search(token).groups()[0] is None:\n",
    "                        print(token)\n",
    "                        # continue\n",
    "                        sys.exit(\"Error: NER is not exits\")\n",
    "\n",
    "                    while nestRegex.search(token) is not None and regex.search(token) is not None:\n",
    "                        textMatch, textMerger, nestData_sent = merger_nest_entities(token)\n",
    "                        # print(\"textMatch = \",textMatch)\n",
    "\n",
    "                        g = token.replace(textMatch, textMerger)\n",
    "                        # print(\"g = \", g)\n",
    "                        token = g\n",
    "\n",
    "                    textMatch, textMerger, nestData_sent = merger_nest_entities(token)\n",
    "                    for d_s in nestData_sent:\n",
    "                        data_sent.append(d_s[0]+\"_\"+d_s[1])\n",
    "                        # print(d_s[0]+\"_\"+d_s[1])\n",
    "\n",
    "            else:\n",
    "                data_sent.append(token)\n",
    "        pass\n",
    "\n",
    "        for i in range(len(data_sent)):\n",
    "            labeledEntities = data_sent[i].split(\"_\")\n",
    "\n",
    "            word = labeledEntities[0]\n",
    "            entities = labeledEntities[1:]\n",
    "            # print(word, entities)\n",
    "            if num_maximum_entities < len(entities):\n",
    "                num_maximum_entities = len(entities)\n",
    "            entities = entities[::-1]\n",
    "\n",
    "            data_sent[i] = [word, entities]\n",
    "        pass\n",
    "        new_data.append(data_sent)\n",
    "        # split_tokenize(d, nestStringRegex)\n",
    "        # print()\n",
    "\n",
    "    for i in range(len(new_data)):\n",
    "        for j in range(len(new_data[i])):\n",
    "            # print(new_data[i])\n",
    "            while len(new_data[i][j][1]) < num_maximum_entities:\n",
    "                new_data[i][j][1].append(\"O\")\n",
    "\n",
    "    return make_text(new_data)\n",
    "\n",
    "\n",
    "def get_sentence_origin(sentence):\n",
    "    # tách các thành phần có chứa \"<ENAMEX ..\" ra khỏi câu:\n",
    "    return re.sub(r'<ENAMEX TYPE=\"[A-Z|\\-]*\">|</ENAMEX>', '', sentence).strip(\" \").replace(\"  \", \" \")\n",
    "\n",
    "\n",
    "def read_input_file(filename):\n",
    "#     if os.path.isfile(filename):\n",
    "#         print(filename)\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = []\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines)):\n",
    "            line = lines[i].strip(\"\\n\")\n",
    "            # if line != \"\":\n",
    "            text.append(line)\n",
    "            # text += lines[i]\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def write_output_file(filename, text):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "def convert(path_in, path_out):\n",
    "    list_files = os.listdir(path_in)\n",
    "    if not os.path.exists(path_out):\n",
    "        os.mkdir(path_out)\n",
    "    for file in tqdm(list_files):\n",
    "        file_path = path_in + \"/\" + file\n",
    "        data = read_input_file(file_path)\n",
    "        output = make_form(data)\n",
    "        file_out = path_out + \"/\" + file[:-3] + \"conll\"\n",
    "        write_output_file(file_out, output)\n",
    "#     write_output_file(path_out, path_out)\n",
    "    print(\"Done!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def handle_word_blacklist(sub_string):\n",
    "  black_token = \"_\"\n",
    "  indexs = [pos for pos, char in enumerate(sub_string) if char == black_token]\n",
    "  for index in indexs:\n",
    "\n",
    "    sub_string = replacer(sub_string, \"-\", index)\n",
    "  return sub_string, indexs\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "def handle_bracket(test_str):\n",
    "  res = re.findall(r'(\\(|\\[|\\\"|\\'|\\{)(.*?)(\\}|\\'|\\\"|\\]|\\))', test_str)\n",
    "  # print(res)\n",
    "  if len(res) > 0:\n",
    "    for r in res:\n",
    "      sub_tring = \"\".join(r)\n",
    "      start_index = test_str.find(sub_tring)\n",
    "      end_index = start_index + len(r[1])\n",
    "      test_str = test_str[: start_index+ 1] + \" \" + test_str[start_index+ 1:]\n",
    "      test_str = test_str[: end_index + 2] + \" \" + test_str[end_index + 2:]\n",
    "      # test_str = \n",
    "  return test_str\n",
    "\n",
    "\n",
    "def handle_character(sub_string):\n",
    "  char_end = [\".\", \",\", \";\", \"?\", \"+\", \":\" ]\n",
    "  count = 1\n",
    "  for index in reversed(range(len(sub_string))):\n",
    "\n",
    "    \n",
    "    # print(index)\n",
    "    c = sub_string[index]\n",
    "    # print(index, c)\n",
    "\n",
    "    #check black token\n",
    "\n",
    "    if c not in char_end:\n",
    "      break\n",
    "    \n",
    "    elif c in char_end:\n",
    "      # print(sent[index -1] )\n",
    "      if sub_string[index -1] not in char_end:\n",
    "        # print(sub_string[index -1])\n",
    "        sub_string = sub_string[:index] + \" \" + sub_string[index:]\n",
    "        count = 2\n",
    "        break\n",
    "\n",
    "  return sub_string, count\n",
    "\n",
    "\n",
    "  def preprocess_data(sent):\n",
    "  sent = handle_bracket(sent)\n",
    "  sent = re.sub(' +', ' ', sent)\n",
    "  sent_out = \"\"\n",
    "  stack = {'ents' : {}}\n",
    "  index_count = 0\n",
    "  parts = sent.split()\n",
    "\n",
    "  for index in range(len(parts)):\n",
    "    word_space = parts[index]\n",
    "    # print(word_space)\n",
    "\n",
    "    sub_string_handeled, count = handle_character(word_space)\n",
    "    \n",
    "    sub_string_handeled, indexs = handle_word_blacklist(sub_string_handeled)\n",
    "    if len(indexs) > 0:\n",
    "      stack['ents'][index_count] = indexs\n",
    "    index_count += count\n",
    "      \n",
    "\n",
    "    if index != len(parts) - 1:\n",
    "      sent_out +=  sub_string_handeled + \" \"\n",
    "    else:\n",
    "      sent_out += sub_string_handeled\n",
    " \n",
    "  stack['sent'] = sent_out\n",
    "  return stack"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "## check dir_out is exist or not\n",
    "## if not, create empty dir_out\n",
    "## input: dir_out (not ended by /)\n",
    "\n",
    "def checkDir(dir_out):\n",
    "    if not os.path.exists(dir_out):\n",
    "        os.mkdir(dir_out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "## replace </ENAMEX>punc => </ENAMEX> punc\n",
    "## input: path of origin dir, path of destination dir (not ended by /)\n",
    "## output: None\n",
    "import string\n",
    "\n",
    "def add_space_tag(dir_in, dir_out):\n",
    "    a = string.punctuation\n",
    "    aa = ['</ENAMEX>' + i for i in a]\n",
    "    aaa = ['</ENAMEX> ' + i for i in  a]\n",
    "\n",
    "    checkDir(dir_out)\n",
    "        \n",
    "    for path_in in tqdm(os.listdir(dir_in)):\n",
    "        dt = []\n",
    "        with open(dir_in + '/' + path_in, 'r', encoding='utf-8') as f:\n",
    "            for i in f.readlines():\n",
    "                for j in range(len(aa)):\n",
    "                    i = i.replace(aa[j], aaa[j])\n",
    "                dt.append(i)\n",
    "        with open(dir_out + '/' + path_in, 'w', encoding='utf-8') as f:\n",
    "            for i in dt:\n",
    "                f.write(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def handle_word_blacklist(sub_string):\n",
    "  black_token = \"_\"\n",
    "  indexs = [pos for pos, char in enumerate(sub_string) if char == black_token]\n",
    "  for index in indexs:\n",
    "\n",
    "    sub_string = replacer(sub_string, \"-\", index)\n",
    "  return sub_string, indexs\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "def handle_bracket(test_str):\n",
    "  res = re.findall(r'(\\(|\\[|\\\"|\\'|\\{)(.*?)(\\}|\\'|\\\"|\\]|\\))', test_str)\n",
    "  # print(res)\n",
    "  if len(res) > 0:\n",
    "    for r in res:\n",
    "      sub_tring = \"\".join(r)\n",
    "      start_index = test_str.find(sub_tring)\n",
    "      end_index = start_index + len(r[1])\n",
    "      test_str = test_str[: start_index+ 1] + \" \" + test_str[start_index+ 1:]\n",
    "      test_str = test_str[: end_index + 2] + \" \" + test_str[end_index + 2:]\n",
    "      # test_str = \n",
    "  return test_str\n",
    "\n",
    "\n",
    "def handle_character(sub_string):\n",
    "  char_end = [\".\", \",\", \";\", \"?\", \"+\", \":\" ]\n",
    "  count = 1\n",
    "  for index in reversed(range(len(sub_string))):\n",
    "\n",
    "    \n",
    "    # print(index)\n",
    "    c = sub_string[index]\n",
    "    # print(index, c)\n",
    "\n",
    "    #check black token\n",
    "\n",
    "    if c not in char_end:\n",
    "      break\n",
    "    \n",
    "    elif c in char_end:\n",
    "      # print(sent[index -1] )\n",
    "      if sub_string[index -1] not in char_end:\n",
    "        # print(sub_string[index -1])\n",
    "        sub_string = sub_string[:index] + \" \" + sub_string[index:]\n",
    "        count = 2\n",
    "        break\n",
    "\n",
    "  return sub_string, count\n",
    "\n",
    "\n",
    "  def preprocess_data(sent):\n",
    "  sent = handle_bracket(sent)\n",
    "  sent = re.sub(' +', ' ', sent)\n",
    "  sent_out = \"\"\n",
    "  stack = {'ents' : {}}\n",
    "  index_count = 0\n",
    "  parts = sent.split()\n",
    "\n",
    "  for index in range(len(parts)):\n",
    "    word_space = parts[index]\n",
    "    # print(word_space)\n",
    "\n",
    "    sub_string_handeled, count = handle_character(word_space)\n",
    "    \n",
    "    sub_string_handeled, indexs = handle_word_blacklist(sub_string_handeled)\n",
    "    if len(indexs) > 0:\n",
    "      stack['ents'][index_count] = indexs\n",
    "    index_count += count\n",
    "      \n",
    "\n",
    "    if index != len(parts) - 1:\n",
    "      sent_out +=  sub_string_handeled + \" \"\n",
    "    else:\n",
    "      sent_out += sub_string_handeled\n",
    " \n",
    "  stack['sent'] = sent_out\n",
    "  return stack"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "## split train, dev, test dts to folder resp: dir_out/train, dir_out/dev, dir_out/test\n",
    "## input: dir_in, dir_out (not ended by /)\n",
    "\n",
    "import shutil\n",
    "def train_dev_test_split(dir_in, dir_out):\n",
    "    checkDir(dir_out)\n",
    "    for i in ['train', 'test', 'dev']:\n",
    "        checkDir(dir_out + '/' + i)\n",
    "    \n",
    "    for i in tqdm(os.listdir(dir_in)):\n",
    "        if 'dev' in i:\n",
    "            shutil.move( dir_in + '/' + i, dir_out + '/dev/' +i)\n",
    "        elif 'test' in i:\n",
    "            shutil.move(dir_in + '/' + i, dir_out + '/test/' + i)\n",
    "        else:\n",
    "            shutil.move(dir_in + '/' + i, dir_out + '/train/' + i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def handle_word_blacklist(sub_string):\n",
    "  black_token = \"_\"\n",
    "  indexs = [pos for pos, char in enumerate(sub_string) if char == black_token]\n",
    "  for index in indexs:\n",
    "\n",
    "    sub_string = replacer(sub_string, \"-\", index)\n",
    "  return sub_string, indexs\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "def handle_bracket(test_str):\n",
    "  res = re.findall(r'(\\(|\\[|\\\"|\\'|\\{)(.*?)(\\}|\\'|\\\"|\\]|\\))', test_str)\n",
    "  # print(res)\n",
    "  if len(res) > 0:\n",
    "    for r in res:\n",
    "      sub_tring = \"\".join(r)\n",
    "      start_index = test_str.find(sub_tring)\n",
    "      end_index = start_index + len(r[1])\n",
    "      test_str = test_str[: start_index+ 1] + \" \" + test_str[start_index+ 1:]\n",
    "      test_str = test_str[: end_index + 2] + \" \" + test_str[end_index + 2:]\n",
    "      # test_str = \n",
    "  return test_str\n",
    "\n",
    "\n",
    "def handle_character(sub_string):\n",
    "  char_end = [\".\", \",\", \";\", \"?\", \"+\", \":\" ]\n",
    "  count = 1\n",
    "  for index in reversed(range(len(sub_string))):\n",
    "\n",
    "    \n",
    "    # print(index)\n",
    "    c = sub_string[index]\n",
    "    # print(index, c)\n",
    "\n",
    "    #check black token\n",
    "\n",
    "    if c not in char_end:\n",
    "      break\n",
    "    \n",
    "    elif c in char_end:\n",
    "      # print(sent[index -1] )\n",
    "      if sub_string[index -1] not in char_end:\n",
    "        # print(sub_string[index -1])\n",
    "        sub_string = sub_string[:index] + \" \" + sub_string[index:]\n",
    "        count = 2\n",
    "        break\n",
    "\n",
    "  return sub_string, count\n",
    "\n",
    "\n",
    "  def preprocess_data(sent):\n",
    "  sent = handle_bracket(sent)\n",
    "  sent = re.sub(' +', ' ', sent)\n",
    "  sent_out = \"\"\n",
    "  stack = {'ents' : {}}\n",
    "  index_count = 0\n",
    "  parts = sent.split()\n",
    "\n",
    "  for index in range(len(parts)):\n",
    "    word_space = parts[index]\n",
    "    # print(word_space)\n",
    "\n",
    "    sub_string_handeled, count = handle_character(word_space)\n",
    "    \n",
    "    sub_string_handeled, indexs = handle_word_blacklist(sub_string_handeled)\n",
    "    if len(indexs) > 0:\n",
    "      stack['ents'][index_count] = indexs\n",
    "    index_count += count\n",
    "      \n",
    "\n",
    "    if index != len(parts) - 1:\n",
    "      sent_out +=  sub_string_handeled + \" \"\n",
    "    else:\n",
    "      sent_out += sub_string_handeled\n",
    " \n",
    "  stack['sent'] = sent_out\n",
    "  return stack"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "## merge all files into only file with name _merge.txt\n",
    "## input: dir_path contain all files need to merge\n",
    "\n",
    "def merge_files(dir_path):\n",
    "    temp = []\n",
    "    for i in os.listdir(dir_path):\n",
    "        with open(dir_path +'/' + i, 'r', encoding='utf-8') as f:\n",
    "            temp += f.readlines()\n",
    "    with open(dir_path + '_merge.txt', 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "## normalize merge file to atleast 6 columns format\n",
    "## input: file_path origin merge, file_path_new\n",
    "\n",
    "def convert2normalize(file_path, file_path_new):\n",
    "    print('convert ' + file_path + 'to atleast 6 columns format')\n",
    "    temp = []\n",
    "    with open( file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) == 3:\n",
    "                line.append('O')\n",
    "            if len(line) >= 4 and len(line) < 6:\n",
    "                for i in range(6-len(line)):\n",
    "                    line.append('?')\n",
    "            if len(line) != 1 and len(line) <4:\n",
    "                line = ['']\n",
    "            temp.append('\\t'.join(line))\n",
    "    with open(file_path_new, 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i + '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "## convert from 6 columns to 2 columns format\n",
    "## input: file_path orgin, file_path_new\n",
    "\n",
    "def convert2columns2(file_path, file_path_new):\n",
    "    print('convert ' + file_path + 'to 2 columns format')\n",
    "    temp = []\n",
    "    with open( file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) > 3:\n",
    "                temp.append('\\t'.join([line[0], line[3]]))\n",
    "            else:\n",
    "                if len(line) > 1:\n",
    "                    print(line, file_path, i)\n",
    "                temp.append('')\n",
    "    with open(file_path_new, 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i + '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "## merge subtags and BI- tags\n",
    "## input: name of tag origin: string\n",
    "## output: name of merge-tag: string\n",
    "\n",
    "def merge_subtag(text):\n",
    "    NER = ['DATETIME', 'PERSONTYPE', 'PERSON', 'ORGANIZATION', 'PRODUCT', 'EVENT', 'LOCATION', 'URL', 'PHONENUMBER', 'QUANTITY', 'IP', 'ADDRESS', 'SKILL', 'EMAIL', 'MISCELLANEOUS']\n",
    "    for i in NER:\n",
    "        if len(i) > 1 and i in text:\n",
    "            return i\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "## convert data 2 columns to merge tag format 2 columns\n",
    "## input: file name origin, file_name new\n",
    "\n",
    "def convert_merge_subtag(file_path, file_new):\n",
    "    print('convert ' + file_path + 'to merge subtag format')\n",
    "    temp = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) > 1:\n",
    "                line[1] = merge_subtag(line[1])\n",
    "            temp.append('\\t'.join(line))\n",
    "    with open(file_new, 'w', encoding='utf-8') as f:\n",
    "        for i in temp:\n",
    "            f.write(i+'\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "## convert from 2 columns format to pkl file\n",
    "## input: file_txt 2 columns format\n",
    "## output: create pickle file with same name (change extension)\n",
    "\n",
    "def convert2pkl(file_txt):\n",
    "    print('convert ' + file_txt + 'to pkl')\n",
    "    temp = []\n",
    "    res = []\n",
    "    with open(file_txt, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                line = line.strip().split('\\t')\n",
    "                temp.append((line[0], line[1]))\n",
    "            else:\n",
    "                if len(temp) > 0:\n",
    "                    res.append(temp)\n",
    "                temp = []\n",
    "    new_file = file_txt.replace('txt', 'pkl')\n",
    "    with open(new_file, 'wb') as f:\n",
    "        pickle.dump(res, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import re\n",
    "\n",
    "def handle_word_blacklist(sub_string):\n",
    "  black_token = \"_\"\n",
    "  indexs = [pos for pos, char in enumerate(sub_string) if char == black_token]\n",
    "  for index in indexs:\n",
    "\n",
    "    sub_string = replacer(sub_string, \"-\", index)\n",
    "  return sub_string, indexs\n",
    "\n",
    "\n",
    "\n",
    "def handle_bracket(test_str):\n",
    "  res = re.findall(r'(\\(|\\[|\\\"|\\'|\\{)(.*?)(\\}|\\'|\\\"|\\]|\\))', test_str)\n",
    "  # print(res)\n",
    "  if len(res) > 0:\n",
    "    for r in res:\n",
    "      sub_tring = \"\".join(r)\n",
    "      start_index = test_str.find(sub_tring)\n",
    "      end_index = start_index + len(r[1])\n",
    "      test_str = test_str[: start_index+ 1] + \" \" + test_str[start_index+ 1:]\n",
    "      test_str = test_str[: end_index + 2] + \" \" + test_str[end_index + 2:]\n",
    "      # test_str = \n",
    "  return test_str\n",
    "\n",
    "def replacer(s, newstring, index, nofail=False):\n",
    "    # raise an error if index is outside of the string\n",
    "    if not nofail and index not in range(len(s)):\n",
    "        raise ValueError(\"index outside given string\")\n",
    "\n",
    "    # if not erroring, but the index is still not in the correct range..\n",
    "    if index < 0:  # add it to the beginning\n",
    "        return newstring + s\n",
    "    if index > len(s):  # add it to the end\n",
    "        return s + newstring\n",
    "\n",
    "    # insert the new string between \"slices\" of the original\n",
    "    return s[:index] + newstring + s[index + 1:]\n",
    "    \n",
    "def handle_character(sub_string):\n",
    "  char_end = [\".\", \",\", \";\", \"?\", \"+\", \":\" ]\n",
    "  count = 1\n",
    "  for index in reversed(range(len(sub_string))):\n",
    "\n",
    "    \n",
    "    # print(index)\n",
    "    c = sub_string[index]\n",
    "    # print(index, c)\n",
    "\n",
    "    #check black token\n",
    "\n",
    "    if c not in char_end:\n",
    "      break\n",
    "    \n",
    "    elif c in char_end:\n",
    "      # print(sent[index -1] )\n",
    "      if sub_string[index -1] not in char_end:\n",
    "        # print(sub_string[index -1])\n",
    "        sub_string = sub_string[:index] + \" \" + sub_string[index:]\n",
    "        count = 2\n",
    "        break\n",
    "\n",
    "  return sub_string, count\n",
    "\n",
    "\n",
    "def preprocess_data(sent):\n",
    "  sent = handle_bracket(sent)\n",
    "  sent = re.sub(' +', ' ', sent)\n",
    "  sent_out = \"\"\n",
    "  stack = {'ents' : {}}\n",
    "  index_count = 0\n",
    "  parts = sent.split()\n",
    "\n",
    "  for index in range(len(parts)):\n",
    "    word_space = parts[index]\n",
    "    # print(word_space)\n",
    "\n",
    "    sub_string_handeled, count = handle_character(word_space)\n",
    "    \n",
    "    sub_string_handeled, indexs = handle_word_blacklist(sub_string_handeled)\n",
    "    if len(indexs) > 0:\n",
    "      stack['ents'][index_count] = indexs\n",
    "    index_count += count\n",
    "      \n",
    "\n",
    "    if index != len(parts) - 1:\n",
    "      sent_out +=  sub_string_handeled + \" \"\n",
    "    else:\n",
    "      sent_out += sub_string_handeled\n",
    "\n",
    "  stack['sent'] = sent_out\n",
    "  return stack"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "text = '''Nguyễn Thị Tình Ô 97 – 98, L_ô E (E13), đường NB17, \"khu dân cư ấp 5\", xã Vĩnh Tân, https://docs.google.com/document/d/1Df7GVVQHfRsGPaqdrhvFi8ftTijrA4Xe25he3vvMiw8/edit Khu công nghiệp VSIP II mở rộng, thị xã Tân Uyên, tỉnh Bình Dương'''\n",
    "preprocess_data(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ents': {8: [1]},\n",
       " 'sent': 'Nguyễn Thị Tình Ô 97 – 98 , L-ô E ( E13 ) , đường NB17 , \" khu dân cư ấp 5 \" , xã Vĩnh Tân , https://docs.google.com/document/d/1Df7GVVQHfRsGPaqdrhvFi8ftTijrA4Xe25he3vvMiw8/edit Khu công nghiệp VSIP II mở rộng , thị xã Tân Uyên , tỉnh Bình Dương'}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "## merge BI tag\n",
    "## input: tag origin: string\n",
    "## output: merge tag: string\n",
    "\n",
    "def merge_BItag(tag):\n",
    "    tag = tag.replace('B-', '')\n",
    "    return tag.replace('I-', '')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "## read dataset from pickle file to dataframe\n",
    "## input: file path pickle, is_merge: boolean (merge BI-tag or not)\n",
    "## output: dataframe\n",
    "\n",
    "def read_dts(file_path, is_merge=True):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        for a, b in i:\n",
    "            X.append(a)\n",
    "            y.append(b)\n",
    "    df = pd.DataFrame(X, columns=['word'])\n",
    "    df['tag'] = y\n",
    "    if is_merge:\n",
    "        df['tag'] = df['tag'].apply(lambda x:merge_BItag(x))\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "## create vocab of dataset with {word: {tag: count of tag}}\n",
    "## input: dataframe\n",
    "## output: vocab: dict\n",
    "\n",
    "def create_vocab_dict(df):\n",
    "    vocab = {}\n",
    "    tag = df.tag.unique()\n",
    "    temp = {}\n",
    "    for i in tag:\n",
    "        temp[i] =  0\n",
    "    for i in range(len(df)):\n",
    "        word, tag = df['word'][i], df['tag'][i]\n",
    "        if word not in vocab:\n",
    "            vocab[word] = temp.copy()\n",
    "        vocab[word][tag] += 1\n",
    "    return vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "colors = ['Apricot', 'Brown', 'Olive', 'Teal', 'Pink', 'Black', 'Red', 'Orange', 'Yellow', 'Lime', 'Green', 'Cyan', 'Blue', 'Purple', \n",
    "         'Grey']\n",
    "NER = ['DATETIME', 'PERSONTYPE', 'PERSON', 'ORGANIZATION', 'PRODUCT', 'EVENT', 'LOCATION', 'URL', 'PHONENUMBER', 'QUANTITY', 'IP', 'ADDRESS', 'SKILL', 'EMAIL', 'MISCELLANEOUS']\n",
    "COLORS = dict()\n",
    "for i in range(len(NER)):\n",
    "    COLORS[NER[i]] = colors[i]\n",
    "OPTIONS = {'ents': NER, 'colors': COLORS}\n",
    "    \n",
    "## visualize result\n",
    "## input: predict format [(word, tag)]\n",
    "\n",
    "def visualize_spacy(arr):\n",
    "    if len(arr) < 1:\n",
    "        return None\n",
    "    text = ' '.join([i for i, j in arr])\n",
    "    pos = 0\n",
    "    start_end_labels = []\n",
    "    for word, tag in arr:\n",
    "        if len(start_end_labels) > 0 and tag == start_end_labels[-1][2]:\n",
    "            temp = [start_end_labels[-1][0], pos+len(word), tag]\n",
    "            start_end_labels[-1] = temp.copy()\n",
    "        else:\n",
    "            temp = [pos, pos+len(word), tag]\n",
    "            start_end_labels.append(temp)\n",
    "        pos += len(word) + 1\n",
    "        \n",
    "    ex = [{'text': text, 'ents': [{'start': x[0], 'end': x[1], 'label': x[2]} for x in start_end_labels if x[2]!= 0]}]\n",
    "    displacy.render(ex, manual=True, jupyter=True, style='ent', options = OPTIONS )  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "## pipeline load dts from all-muc-folder\n",
    "LIST_FOLDER = ['train', 'dev', 'test']\n",
    "DIR_MUC = 'MyNER21/NER-Data-Muc' ## contain all origin muc files, linked to your dataset.\n",
    "DIR_MUC_FIXED = 'demo_fixed'  ## contain all muc files fixed space, any name.\n",
    "DIR_CONLL =  'demo_conll' ## contain all conll files, any name.\n",
    "DIR_DTS =   'demo_dts' ## contain folders train, dev, test which each folder contains files conll after splitting.\n",
    "print('Add space after TAG HTML END and punc:')\n",
    "add_space_tag(DIR_MUC, DIR_MUC_FIXED)  ## add space after </ENAMEX>punc\n",
    "print('Convert muc format to conll format:')\n",
    "convert(DIR_MUC_FIXED, DIR_CONLL)  ## convert muc format to conll format\n",
    "print('Train dev test split:')\n",
    "train_dev_test_split(DIR_CONLL, DIR_DTS) ## split train, dev, test\n",
    "for i in LIST_FOLDER:\n",
    "    merge_files(DIR_DTS + '/' + i)     ## merge folder train, dev, test into only files with name DIR_DTS/train_merge.txt, DIR_DTS/dev_merge.txt, DIR_DTS/test_merge.txt\n",
    "    convert2normalize(DIR_DTS+'/'+i+'_merge.txt', DIR_DTS+'/'+i+'_fixed.txt')\n",
    "    convert2columns2(DIR_DTS+'/'+i+'_fixed.txt', DIR_DTS+'/'+i+'_2columns.txt')\n",
    "    convert_merge_subtag(DIR_DTS+'/'+i+'_2columns.txt', DIR_DTS+'/'+i+'_merge_subtag.txt')\n",
    "    convert2pkl(DIR_DTS+'/'+i+'_merge_subtag.txt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:00<00:00, 3133.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:19<00:00, 78.88it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:00<00:00, 7762.74it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from IPython.display import HTML as html_print\n",
    "\n",
    "COLORS = ['Apricot', 'Brown', 'Olive', 'Teal', 'Pink', 'Black', 'Red', 'Orange', 'Yellow', 'Lime', 'Green', 'Cyan', 'Blue', 'Purple', \n",
    "         'Grey']    \n",
    "\n",
    "def background_color(s, text_color='Red', bg_color='Beige'):\n",
    "    return \"<text style='background-color:{}; color:{}; font-size:150%'><b> {} </b></text>\".format(bg_color, text_color,s)\n",
    "\n",
    "## check b(b1, b2) in a(a1, a2)\n",
    "def isBelongRange(a, b):\n",
    "     return b[0] >= a[0] and b[1] <= a[1]\n",
    "    \n",
    "def sorted_idx(idx, isReverse = False):\n",
    "    temp = [(u, v, a) for u, v, a in sorted(idx, key=lambda item: item[0], reverse = isReverse)]\n",
    "    res = [temp[0]]\n",
    "    for i in range(1, len(temp)):\n",
    "        if isBelongRange(temp[i], temp[i-1]):\n",
    "            res[-1] = temp[i]\n",
    "        elif not isBelongRange(temp[i-1], temp[i]):\n",
    "            res.append(temp[i])\n",
    "    return res\n",
    "            \n",
    "\n",
    "def find_all_idx(text, keyword, isLowerUpper=False):\n",
    "    if not isLowerUpper:\n",
    "        ntext = text.lower()\n",
    "        nkeyword = [i.lower() for i in keyword]\n",
    "    else:\n",
    "        ntext, nkeyword = text.copy(), keyword.copy()\n",
    "    idx = []\n",
    "    for a, i in enumerate(nkeyword):\n",
    "        idx += [(m.start(), m.end(), str(a)) for m in re.finditer(i, ntext)]\n",
    "    return sorted_idx(idx)\n",
    "\n",
    "## visualize format html - Tan's format\n",
    "def visualize_keyword_html(text, keyword, isLowerUpper=False):\n",
    "    idx = find_all_idx(text, keyword, isLowerUpper)\n",
    "    pos, cur = 0, 0\n",
    "    lst = []\n",
    "    while (pos < len(text) and cur < len(idx)):\n",
    "        if pos < idx[cur][0]:\n",
    "            lst.append(text[pos: idx[cur][0]])\n",
    "            pos = idx[cur][0]\n",
    "        else:\n",
    "            lst.append(background_color(text[pos: idx[cur][1]], COLORS[int(idx[cur][2])]))\n",
    "            pos = idx[cur][1]\n",
    "            cur += 1\n",
    "    if pos < len(text):\n",
    "        lst.append(text[pos:])\n",
    "    return html_print(' '.join(lst))\n",
    "        \n",
    "## visualize format spacy - Tinh's format\n",
    "def visualize_keyword_spacy(text, keyword, isLowerUpper=False):\n",
    "    idx = find_all_idx(text, keyword, isLowerUpper)\n",
    "    colors = dict()\n",
    "    for i in range(len(keyword)):\n",
    "        colors[str(i)] = COLORS[i%len(COLORS)]\n",
    "    OPTIONS = {'colors': colors}\n",
    "\n",
    "    ex = [{'text': text, 'ents': [{'start': x[0], 'end': x[1], 'label': x[2]} for x in idx]}]\n",
    "    displacy.render(ex, manual=True, jupyter=True, style='ent', options=OPTIONS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text = 'Các cụ thường dạy con cháu rằng \"Chớ eo xèo khi đãi khách , đừng hậm hực lúc ăn cơm\" là nhắc nhở phép tắc cư xử \\\n",
    "khi ngồi bên mâm cơm nhà . Tuy nhiên , mọi việc xảy ra ngoài ý muốn cũng xuất phát từ mâm cơm mà ra . Mới đây , \\\n",
    "những hình ảnh và dòng trạng thái đắng lòng của một thành viên mạng xã hội về mâm cơm thừa nhà chồng để lại \\\n",
    "cho con dâu phần khiến chị em khó tránh được cảm giác tủi thân'\n",
    "kw = ['con', 'con cháu', 'khi', 'và', 'thân']\n",
    "\n",
    "visualize_keyword_html(text, kw)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "Các cụ thường dạy  <text style='background-color:Beige; color:Brown; font-size:150%'><b> con cháu </b></text>  rằng \"Chớ eo xèo  <text style='background-color:Beige; color:Olive; font-size:150%'><b> khi </b></text>  đãi khách , đừng hậm hực lúc ăn cơm\" là nhắc nhở phép tắc cư xử  <text style='background-color:Beige; color:Olive; font-size:150%'><b> khi </b></text>  ngồi bên mâm cơm nhà . Tuy nhiên , mọi việc xảy ra ngoài ý muốn cũng xuất phát từ mâm cơm mà ra . Mới đây , những hình ảnh  <text style='background-color:Beige; color:Teal; font-size:150%'><b> và </b></text>  dòng trạng thái đắng lòng của một thành viên mạng xã hội về mâm cơm thừa nhà chồng để lại cho  <text style='background-color:Beige; color:Apricot; font-size:150%'><b> con </b></text>  dâu phần  <text style='background-color:Beige; color:Olive; font-size:150%'><b> khi </b></text> ến chị em khó tránh được cảm giác tủi  <text style='background-color:Beige; color:Pink; font-size:150%'><b> thân </b></text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "visualize_keyword_spacy(text, kw)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Các cụ thường dạy \n",
       "<mark class=\"entity\" style=\"background: Brown; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    con cháu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">1</span>\n",
       "</mark>\n",
       " rằng &quot;Chớ eo xèo \n",
       "<mark class=\"entity\" style=\"background: Olive; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    khi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">2</span>\n",
       "</mark>\n",
       " đãi khách , đừng hậm hực lúc ăn cơm&quot; là nhắc nhở phép tắc cư xử \n",
       "<mark class=\"entity\" style=\"background: Olive; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    khi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">2</span>\n",
       "</mark>\n",
       " ngồi bên mâm cơm nhà . Tuy nhiên , mọi việc xảy ra ngoài ý muốn cũng xuất phát từ mâm cơm mà ra . Mới đây , những hình ảnh \n",
       "<mark class=\"entity\" style=\"background: Teal; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    và\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">3</span>\n",
       "</mark>\n",
       " dòng trạng thái đắng lòng của một thành viên mạng xã hội về mâm cơm thừa nhà chồng để lại cho \n",
       "<mark class=\"entity\" style=\"background: Apricot; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    con\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0</span>\n",
       "</mark>\n",
       " dâu phần \n",
       "<mark class=\"entity\" style=\"background: Olive; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    khi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">2</span>\n",
       "</mark>\n",
       "ến chị em khó tránh được cảm giác tủi \n",
       "<mark class=\"entity\" style=\"background: Pink; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    thân\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">4</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('myenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "interpreter": {
   "hash": "e340ebf6e4c46a1dde7bff8fb86e91f7b447ced65d6f8968eec1f1e9833f02ab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}